{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f38f54-0651-447d-8513-7ede07d0ff06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Working with JSON file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da1e98c-3698-4025-95ab-014dae4f3193",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ARR_TIME</th><th>CANCELLED</th><th>CRS_ARR_TIME</th><th>CRS_DEP_TIME</th><th>DEP_TIME</th><th>DEST</th><th>DEST_CITY_NAME</th><th>DISTANCE</th><th>FL_DATE</th><th>OP_CARRIER</th><th>OP_CARRIER_FL_NUM</th><th>ORIGIN</th><th>ORIGIN_CITY_NAME</th><th>TAXI_IN</th><th>WHEELS_ON</th></tr></thead><tbody><tr><td>1348</td><td>0</td><td>1400</td><td>1115</td><td>1113</td><td>ATL</td><td>Atlanta, GA</td><td>946</td><td>1/1/2000</td><td>DL</td><td>1451</td><td>BOS</td><td>Boston, MA</td><td>5</td><td>1343</td></tr><tr><td>1543</td><td>0</td><td>1559</td><td>1315</td><td>1311</td><td>ATL</td><td>Atlanta, GA</td><td>946</td><td>1/1/2000</td><td>DL</td><td>1479</td><td>BOS</td><td>Boston, MA</td><td>7</td><td>1536</td></tr><tr><td>1651</td><td>0</td><td>1721</td><td>1415</td><td>1414</td><td>ATL</td><td>Atlanta, GA</td><td>946</td><td>1/1/2000</td><td>DL</td><td>1857</td><td>BOS</td><td>Boston, MA</td><td>9</td><td>1642</td></tr><tr><td>2005</td><td>0</td><td>2013</td><td>1715</td><td>1720</td><td>ATL</td><td>Atlanta, GA</td><td>946</td><td>1/1/2000</td><td>DL</td><td>1997</td><td>BOS</td><td>Boston, MA</td><td>10</td><td>1955</td></tr><tr><td>2240</td><td>0</td><td>2300</td><td>2015</td><td>2010</td><td>ATL</td><td>Atlanta, GA</td><td>946</td><td>1/1/2000</td><td>DL</td><td>2065</td><td>BOS</td><td>Boston, MA</td><td>10</td><td>2230</td></tr><tr><td>1003</td><td>0</td><td>955</td><td>650</td><td>649</td><td>ATL</td><td>Atlanta, GA</td><td>946</td><td>1/1/2000</td><td>US</td><td>2619</td><td>BOS</td><td>Boston, MA</td><td>7</td><td>956</td></tr><tr><td>1717</td><td>0</td><td>1738</td><td>1440</td><td>1446</td><td>ATL</td><td>Atlanta, GA</td><td>946</td><td>1/1/2000</td><td>US</td><td>2621</td><td>BOS</td><td>Boston, MA</td><td>4</td><td>1713</td></tr><tr><td>2006</td><td>0</td><td>2008</td><td>1740</td><td>1744</td><td>ATL</td><td>Atlanta, GA</td><td>449</td><td>1/1/2000</td><td>DL</td><td>346</td><td>BTR</td><td>Baton Rouge, LA</td><td>9</td><td>1957</td></tr><tr><td>1601</td><td>0</td><td>1622</td><td>1345</td><td>1345</td><td>ATL</td><td>Atlanta, GA</td><td>449</td><td>1/1/2000</td><td>DL</td><td>412</td><td>BTR</td><td>Baton Rouge, LA</td><td>9</td><td>1552</td></tr><tr><td>1448</td><td>0</td><td>1455</td><td>1245</td><td>1245</td><td>ATL</td><td>Atlanta, GA</td><td>712</td><td>1/1/2000</td><td>DL</td><td>299</td><td>BUF</td><td>Buffalo, NY</td><td>5</td><td>1443</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1348,
         0,
         1400,
         1115,
         1113,
         "ATL",
         "Atlanta, GA",
         946,
         "1/1/2000",
         "DL",
         1451,
         "BOS",
         "Boston, MA",
         5,
         1343
        ],
        [
         1543,
         0,
         1559,
         1315,
         1311,
         "ATL",
         "Atlanta, GA",
         946,
         "1/1/2000",
         "DL",
         1479,
         "BOS",
         "Boston, MA",
         7,
         1536
        ],
        [
         1651,
         0,
         1721,
         1415,
         1414,
         "ATL",
         "Atlanta, GA",
         946,
         "1/1/2000",
         "DL",
         1857,
         "BOS",
         "Boston, MA",
         9,
         1642
        ],
        [
         2005,
         0,
         2013,
         1715,
         1720,
         "ATL",
         "Atlanta, GA",
         946,
         "1/1/2000",
         "DL",
         1997,
         "BOS",
         "Boston, MA",
         10,
         1955
        ],
        [
         2240,
         0,
         2300,
         2015,
         2010,
         "ATL",
         "Atlanta, GA",
         946,
         "1/1/2000",
         "DL",
         2065,
         "BOS",
         "Boston, MA",
         10,
         2230
        ],
        [
         1003,
         0,
         955,
         650,
         649,
         "ATL",
         "Atlanta, GA",
         946,
         "1/1/2000",
         "US",
         2619,
         "BOS",
         "Boston, MA",
         7,
         956
        ],
        [
         1717,
         0,
         1738,
         1440,
         1446,
         "ATL",
         "Atlanta, GA",
         946,
         "1/1/2000",
         "US",
         2621,
         "BOS",
         "Boston, MA",
         4,
         1713
        ],
        [
         2006,
         0,
         2008,
         1740,
         1744,
         "ATL",
         "Atlanta, GA",
         449,
         "1/1/2000",
         "DL",
         346,
         "BTR",
         "Baton Rouge, LA",
         9,
         1957
        ],
        [
         1601,
         0,
         1622,
         1345,
         1345,
         "ATL",
         "Atlanta, GA",
         449,
         "1/1/2000",
         "DL",
         412,
         "BTR",
         "Baton Rouge, LA",
         9,
         1552
        ],
        [
         1448,
         0,
         1455,
         1245,
         1245,
         "ATL",
         "Atlanta, GA",
         712,
         "1/1/2000",
         "DL",
         299,
         "BUF",
         "Buffalo, NY",
         5,
         1443
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ARR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CANCELLED",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_ARR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_DEP_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEP_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DEST",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEST_CITY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DISTANCE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "FL_DATE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "OP_CARRIER",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "OP_CARRIER_FL_NUM",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_CITY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TAXI_IN",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "WHEELS_ON",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "flight_raw_df = spark.read\\\n",
    "    .format(\"json\")\\\n",
    "    .load(\"/Volumes/dev/multi_datasets/spark_data/flight-time.json\")\n",
    "display(flight_raw_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "893fe85d-6fe7-4fae-82c8-e479c79742f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- ARR_TIME: long (nullable = true)\n |-- CANCELLED: long (nullable = true)\n |-- CRS_ARR_TIME: long (nullable = true)\n |-- CRS_DEP_TIME: long (nullable = true)\n |-- DEP_TIME: long (nullable = true)\n |-- DEST: string (nullable = true)\n |-- DEST_CITY_NAME: string (nullable = true)\n |-- DISTANCE: long (nullable = true)\n |-- FL_DATE: string (nullable = true)\n |-- OP_CARRIER: string (nullable = true)\n |-- OP_CARRIER_FL_NUM: long (nullable = true)\n |-- ORIGIN: string (nullable = true)\n |-- ORIGIN_CITY_NAME: string (nullable = true)\n |-- TAXI_IN: long (nullable = true)\n |-- WHEELS_ON: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "flight_raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22c8995d-6a55-4d44-8e96-4167a0ff134e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Automatic inferring schema is often incorrect. As in this case FL_DATE column is string but in real it is date, Cancelled column is boolean but here it is read as long. SO now we will enforce our own schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8961c52f-01d6-464a-8c22-01029d7fc037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flight_schema = \"\"\"\n",
    "    FL_DATE date,\n",
    "    OP_CARRIER string,\n",
    "    OP_CARRIER_FL_NUM long,\n",
    "    ORIGIN string,\n",
    "    ORIGIN_CITY_NAME string,\n",
    "    DEST string,\n",
    "    DEST_CITY_NAME string,\n",
    "    DEP_TIME long,\n",
    "    ARR_TIME long,\n",
    "    CRS_DEP_TIME long,\n",
    "    CRS_ARR_TIME long,\n",
    "    DISTANCE long,\n",
    "    CANCELLED boolean,\n",
    "    TAXI_IN long,\n",
    "    WHEELS_ON long\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86805989-906c-4131-ab36-a606f7079220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>FL_DATE</th><th>OP_CARRIER</th><th>OP_CARRIER_FL_NUM</th><th>ORIGIN</th><th>ORIGIN_CITY_NAME</th><th>DEST</th><th>DEST_CITY_NAME</th><th>DEP_TIME</th><th>ARR_TIME</th><th>CRS_DEP_TIME</th><th>CRS_ARR_TIME</th><th>DISTANCE</th><th>CANCELLED</th><th>TAXI_IN</th><th>WHEELS_ON</th></tr></thead><tbody><tr><td>null</td><td>DL</td><td>1451</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1113</td><td>1348</td><td>1115</td><td>1400</td><td>946</td><td>null</td><td>5</td><td>1343</td></tr><tr><td>null</td><td>DL</td><td>1479</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1311</td><td>1543</td><td>1315</td><td>1559</td><td>946</td><td>null</td><td>7</td><td>1536</td></tr><tr><td>null</td><td>DL</td><td>1857</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1414</td><td>1651</td><td>1415</td><td>1721</td><td>946</td><td>null</td><td>9</td><td>1642</td></tr><tr><td>null</td><td>DL</td><td>1997</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1720</td><td>2005</td><td>1715</td><td>2013</td><td>946</td><td>null</td><td>10</td><td>1955</td></tr><tr><td>null</td><td>DL</td><td>2065</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>2010</td><td>2240</td><td>2015</td><td>2300</td><td>946</td><td>null</td><td>10</td><td>2230</td></tr><tr><td>null</td><td>US</td><td>2619</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>649</td><td>1003</td><td>650</td><td>955</td><td>946</td><td>null</td><td>7</td><td>956</td></tr><tr><td>null</td><td>US</td><td>2621</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1446</td><td>1717</td><td>1440</td><td>1738</td><td>946</td><td>null</td><td>4</td><td>1713</td></tr><tr><td>null</td><td>DL</td><td>346</td><td>BTR</td><td>Baton Rouge, LA</td><td>ATL</td><td>Atlanta, GA</td><td>1744</td><td>2006</td><td>1740</td><td>2008</td><td>449</td><td>null</td><td>9</td><td>1957</td></tr><tr><td>null</td><td>DL</td><td>412</td><td>BTR</td><td>Baton Rouge, LA</td><td>ATL</td><td>Atlanta, GA</td><td>1345</td><td>1601</td><td>1345</td><td>1622</td><td>449</td><td>null</td><td>9</td><td>1552</td></tr><tr><td>null</td><td>DL</td><td>299</td><td>BUF</td><td>Buffalo, NY</td><td>ATL</td><td>Atlanta, GA</td><td>1245</td><td>1448</td><td>1245</td><td>1455</td><td>712</td><td>null</td><td>5</td><td>1443</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         "DL",
         1451,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1113,
         1348,
         1115,
         1400,
         946,
         null,
         5,
         1343
        ],
        [
         null,
         "DL",
         1479,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1311,
         1543,
         1315,
         1559,
         946,
         null,
         7,
         1536
        ],
        [
         null,
         "DL",
         1857,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1414,
         1651,
         1415,
         1721,
         946,
         null,
         9,
         1642
        ],
        [
         null,
         "DL",
         1997,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1720,
         2005,
         1715,
         2013,
         946,
         null,
         10,
         1955
        ],
        [
         null,
         "DL",
         2065,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         2010,
         2240,
         2015,
         2300,
         946,
         null,
         10,
         2230
        ],
        [
         null,
         "US",
         2619,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         649,
         1003,
         650,
         955,
         946,
         null,
         7,
         956
        ],
        [
         null,
         "US",
         2621,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1446,
         1717,
         1440,
         1738,
         946,
         null,
         4,
         1713
        ],
        [
         null,
         "DL",
         346,
         "BTR",
         "Baton Rouge, LA",
         "ATL",
         "Atlanta, GA",
         1744,
         2006,
         1740,
         2008,
         449,
         null,
         9,
         1957
        ],
        [
         null,
         "DL",
         412,
         "BTR",
         "Baton Rouge, LA",
         "ATL",
         "Atlanta, GA",
         1345,
         1601,
         1345,
         1622,
         449,
         null,
         9,
         1552
        ],
        [
         null,
         "DL",
         299,
         "BUF",
         "Buffalo, NY",
         "ATL",
         "Atlanta, GA",
         1245,
         1448,
         1245,
         1455,
         712,
         null,
         5,
         1443
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "FL_DATE",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "OP_CARRIER",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "OP_CARRIER_FL_NUM",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_CITY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEST",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEST_CITY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEP_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_DEP_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_ARR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DISTANCE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CANCELLED",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "TAXI_IN",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "WHEELS_ON",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- FL_DATE: date (nullable = true)\n |-- OP_CARRIER: string (nullable = true)\n |-- OP_CARRIER_FL_NUM: long (nullable = true)\n |-- ORIGIN: string (nullable = true)\n |-- ORIGIN_CITY_NAME: string (nullable = true)\n |-- DEST: string (nullable = true)\n |-- DEST_CITY_NAME: string (nullable = true)\n |-- DEP_TIME: long (nullable = true)\n |-- ARR_TIME: long (nullable = true)\n |-- CRS_DEP_TIME: long (nullable = true)\n |-- CRS_ARR_TIME: long (nullable = true)\n |-- DISTANCE: long (nullable = true)\n |-- CANCELLED: boolean (nullable = true)\n |-- TAXI_IN: long (nullable = true)\n |-- WHEELS_ON: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "flight_raw_df1 = spark.read\\\n",
    "    .format(\"json\")\\\n",
    "    .schema(flight_schema)\\\n",
    "    .load(\"/Volumes/dev/multi_datasets/spark_data/flight-time.json\")\n",
    "display(flight_raw_df1.limit(10))\n",
    "flight_raw_df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e5cde51-218e-4fcc-a238-fc9baa3f7fae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**The two columns FL_DATE and CANCELLED column are showing null values now as the dataframe reader could not load the data correctly. Hence the date, timestamp and the boolean values are often loaded incorrectly or they become null.**\n",
    "\n",
    "**Json file has Flight date in MM/DD/YYYY format but default date format is YYYY-MM-DD format. JSON file has boolean as 0 & 1 but default value is TRUE & FALSE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d843c2e0-22b0-416d-80ea-e7632038db09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**So we will set a error message for this using dataframe reader mode option- FAILFAST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ca50f63-1352-42b9-961f-65d77ad5c844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mSparkException\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4855986274339412>, line 6\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m flight_raw_df1 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\\\n",
       "\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mschema(flight_schema)\\\n",
       "\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFAILFAST\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n",
       "\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Volumes/dev/multi_datasets/spark_data/flight-time.json\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m----> 6\u001B[0m display(flight_raw_df1\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m10\u001B[39m))\n",
       "\u001B[1;32m      7\u001B[0m flight_raw_df1\u001B[38;5;241m.\u001B[39mprintSchema()\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:133\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
       "\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cf_helper \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n",
       "\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay_connect_table(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n",
       "\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39misStreaming:\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:97\u001B[0m, in \u001B[0;36mDisplay.display_connect_table\u001B[0;34m(self, df, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     94\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_streaming_dataframe(df, config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstreaming_listener,\n",
       "\u001B[1;32m     95\u001B[0m                                                \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m---> 97\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_dataframe(df, config)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:48\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.display_dataframe\u001B[0;34m(self, df, config)\u001B[0m\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdisplay_dataframe\u001B[39m(\u001B[38;5;28mself\u001B[39m, df: ConnectDataFrame, config: TableDisplayConfig):\n",
       "\u001B[0;32m---> 48\u001B[0m     display_payload \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite_to_cloudfetch(df, config)\n",
       "\u001B[1;32m     49\u001B[0m     ip_display({\n",
       "\u001B[1;32m     50\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/vnd.databricks.connect.display\u001B[39m\u001B[38;5;124m\"\u001B[39m: display_payload,\n",
       "\u001B[1;32m     51\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext/plain\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatabricks Spark Connect Table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     52\u001B[0m     },\n",
       "\u001B[1;32m     53\u001B[0m                raw\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:134\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n",
       "\u001B[1;32m    132\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m PySparkException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[0;32m--> 134\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    136\u001B[0m     error_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtype\u001B[39m(\n",
       "\u001B[1;32m    137\u001B[0m         e\n",
       "\u001B[1;32m    138\u001B[0m     )(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEncountered an unexpected error when displaying table. Try again later, or detaching and reattaching to your compute.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    139\u001B[0m       )\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:103\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n",
       "\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m rowLimit \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    101\u001B[0m     connectDataFrame \u001B[38;5;241m=\u001B[39m cast(ConnectDataFrame, connectDataFrame\u001B[38;5;241m.\u001B[39mlimit(rowLimit \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m))\n",
       "\u001B[1;32m    102\u001B[0m results: Tuple[Optional[StructType], List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCloudFetchResult\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n",
       "\u001B[0;32m--> 103\u001B[0m                List[\u001B[38;5;28mbool\u001B[39m]] \u001B[38;5;241m=\u001B[39m connectDataFrame\u001B[38;5;241m.\u001B[39m_to_cloudfetch_with_limits_and_file_paths(\n",
       "\u001B[1;32m    104\u001B[0m                    \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39mstr_format,\n",
       "\u001B[1;32m    105\u001B[0m                    compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    106\u001B[0m                    row_limit\u001B[38;5;241m=\u001B[39mrowLimit,\n",
       "\u001B[1;32m    107\u001B[0m                    byte_limit\u001B[38;5;241m=\u001B[39mbyteLimit)\n",
       "\u001B[1;32m    108\u001B[0m pyspark_struct, cloudfetch_results, batch_truncation_results \u001B[38;5;241m=\u001B[39m results\n",
       "\u001B[1;32m    110\u001B[0m schema \u001B[38;5;241m=\u001B[39m CloudFetchDisplayHelper\u001B[38;5;241m.\u001B[39mget_display_schema_from_pyspark_struct(pyspark_struct)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1882\u001B[0m, in \u001B[0;36mDataFrame._to_cloudfetch_with_limits_and_file_paths\u001B[0;34m(self, format, compression, row_limit, byte_limit)\u001B[0m\n",
       "\u001B[1;32m   1861\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1862\u001B[0m \u001B[38;5;124;03mThis is an experimental method to support generating results in the form of pre-signed\u001B[39;00m\n",
       "\u001B[1;32m   1863\u001B[0m \u001B[38;5;124;03mURLs that can be used by the client to fetch the results directly from cloud storage.\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1879\u001B[0m \n",
       "\u001B[1;32m   1880\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1881\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 1882\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexperimental_to_cloudfetch(\n",
       "\u001B[1;32m   1883\u001B[0m     query, \u001B[38;5;28mformat\u001B[39m, compression, row_limit, byte_limit, \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   1884\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1308\u001B[0m, in \u001B[0;36mSparkConnectClient.experimental_to_cloudfetch\u001B[0;34m(self, plan, format, compression, row_limit, byte_limit, with_internal_file_path)\u001B[0m\n",
       "\u001B[1;32m   1305\u001B[0m \u001B[38;5;66;03m# Execute the request and iterate over the streaming results. We capture the schema\u001B[39;00m\n",
       "\u001B[1;32m   1306\u001B[0m \u001B[38;5;66;03m# response and unpack the CloudResultBatch responses.\u001B[39;00m\n",
       "\u001B[1;32m   1307\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 1308\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(req, {}, [], progress\u001B[38;5;241m=\u001B[39mprogress):\n",
       "\u001B[1;32m   1309\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1310\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2434\u001B[0m                 info,\n",
       "\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2438\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mSparkException\u001B[0m: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [null,DL,1451,BOS,Boston, MA,ATL,Atlanta, GA,1113,1348,1115,1400,946,null,5,1343].\n",
       "Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.  SQLSTATE: 22023\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.SparkException\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:2162)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.throwMalformedRecordsDetectedInRecordParsingError(FailureSafeParser.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:122)\n",
       "\tat com.databricks.photon.NativeJsonReaderHelper$.$anonfun$readSingleLine$3(NativeJsonReaderHelper.scala:108)\n",
       "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
       "\tat scala.collection.convert.JavaCollectionWrappers$IteratorWrapper.hasNext(JavaCollectionWrappers.scala:32)\n",
       "\tat com.databricks.photon.NativeReader.next(NativeReader.java:108)\n",
       "\tat 0xc331ca7 <photon>.Next(external/workspace_spark_4_0/photon/jni-wrappers/jni-reader.cc:110)\n",
       "\tat 0x7598e8b <photon>.ReaderBatch(external/workspace_spark_4_0/photon/exec-nodes/common-file-scan-node.h:224)\n",
       "\tat 0x7598833 <photon>.NextImpl(external/workspace_spark_4_0/photon/exec-nodes/json-file-scan-node.cc:140)\n",
       "\tat 0x7494007 <photon>.Next(external/workspace_spark_4_0/photon/exec-nodes/exec-node.cc:222)\n",
       "\tat com.databricks.photon.JniApiImpl.getNext(JniApi.scala:-2)\n",
       "\tat com.databricks.photon.JniApi.getNext(JniApi.scala:-1)\n",
       "\tat com.databricks.photon.JniExecNode.next(JniExecNode.java:84)\n",
       "\tat com.databricks.photon.PhotonColumnarBatchResultHandler$$anon$2.next(PhotonExec.scala:1508)\n",
       "\tat com.databricks.photon.PhotonColumnarBatchResultHandler$$anon$2.next(PhotonExec.scala:1504)\n",
       "\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$next$2(PhotonBasicEvaluatorFactory.scala:293)\n",
       "\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n",
       "\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n",
       "\tat com.databricks.photon.PhotonColumnarBatchResultHandler.timeit(PhotonExec.scala:1492)\n",
       "\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$next$1(PhotonBasicEvaluatorFactory.scala:293)\n",
       "\tat com.databricks.photon.metrics.BillableTimeTaskMetrics.withPhotonBilling(BillableTimeTaskMetrics.scala:71)\n",
       "\tat org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:273)\n",
       "\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.next(PhotonBasicEvaluatorFactory.scala:291)\n",
       "\tat com.databricks.photon.CloseableIterator$$anon$10.next(CloseableIterator.scala:212)\n",
       "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n",
       "\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:1240)\n",
       "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:636)\n",
       "\tat com.databricks.sql.execution.arrowcollect.RowBasedConverters$$anon$1.<init>(RowBasedConverters.scala:110)\n",
       "\tat com.databricks.sql.execution.arrowcollect.RowBasedConverters$.toHybridResultIterator(RowBasedConverters.scala:99)\n",
       "\tat com.databricks.sql.execution.arrowcollect.HybridJsonArrayCollector$.convertRows(HybridJsonArrayCollector.scala:66)\n",
       "\tat com.databricks.sql.execution.arrowcollect.HybridJsonArrayCollector$.$anonfun$executorCollect$1(HybridJsonArrayCollector.scala:135)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:954)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:954)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:62)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:429)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:426)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:393)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n",
       "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:233)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:197)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:160)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.scheduler.TaskExecutionUtils$.withUCHandleForTaskExecution(Task.scala:355)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$3(Task.scala:154)\n",
       "\tat org.apache.spark.scheduler.TaskExecutionUtils$.withCredentialsForTaskExecution(Task.scala:333)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:113)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1255)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1259)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1108)\n",
       "\tat com.databricks.aether.RDDTask.run(RDDTask.scala:252)\n",
       "\tat com.databricks.aether.worker.WorkerTaskAttemptThread.$anonfun$runInternal$1(AetherWorkerImpl.scala:362)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.aether.AetherInt64Gauge.scopedAdd(AetherServiceMetricImpls.scala:129)\n",
       "\tat com.databricks.aether.worker.WorkerTaskAttemptThread.runInternal(AetherWorkerImpl.scala:351)\n",
       "\tat com.databricks.aether.worker.WorkerTaskAttemptThread.run(AetherWorkerImpl.scala:328)\n",
       "\tat com.databricks.aether.FairBlockingQueue$SlotCountingRunnable.run(FairBlockingQueue.java:638)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.lang.Thread.run(Thread.java:840)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1709)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1693)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3288)\n",
       "\tat com.databricks.sql.execution.arrowcollect.RDDBatchCollector.$anonfun$runSparkJobs$2(RDDBatchCollector.scala:264)\n",
       "\tat com.databricks.sql.execution.arrowcollect.RDDBatchCollector.$anonfun$runSparkJobs$2$adapted(RDDBatchCollector.scala:261)\n",
       "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
       "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1306)\n",
       "\tat com.databricks.sql.execution.arrowcollect.RDDBatchCollector.runSparkJobs(RDDBatchCollector.scala:261)\n",
       "\tat com.databricks.sql.execution.arrowcollect.RDDBatchCollector.collect(RDDBatchCollector.scala:347)\n",
       "\tat com.databricks.sql.execution.arrowcollect.CloudStoreCollector$.hybridCollect(CloudStoreCollector.scala:159)\n",
       "\tat com.databricks.sql.execution.arrowcollect.CloudStoreCollector$.hybridCollect(CloudStoreCollector.scala:206)\n",
       "\tat org.apache.spark.sql.execution.qrc.HybridCloudStoreFormat.collect(cachedSparkResults.scala:149)\n",
       "\tat org.apache.spark.sql.execution.qrc.HybridCloudStoreFormat.collect(cachedSparkResults.scala:139)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:613)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:607)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:624)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:448)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeHybridCloudStoreResult(ResultCacheManager.scala:407)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeHybridCloudStoreCollectResult(SparkPlan.scala:631)\n",
       "\tat org.apache.spark.sql.classic.Dataset.$anonfun$collectToHybridCloudStoreResults$1(Dataset.scala:1837)\n",
       "\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$4(Dataset.scala:2766)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n",
       "\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$3(Dataset.scala:2764)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n",
       "\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2764)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)\n",
       "\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2763)\n",
       "\tat org.apache.spark.sql.classic.Dataset.collectToHybridCloudStoreResults(Dataset.scala:1836)\n",
       "\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.processAsRemoteBatches(SparkConnectPlanExecution.scala:570)\n",
       "\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:149)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:376)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)\n",
       "Caused by: org.apache.spark.SparkDateTimeException: [CAST_INVALID_INPUT] The value '1/1/2000' of the type \"STRING\" cannot be cast to \"DATE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
       "\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeErrorInternal(ExecutionErrors.scala:119)\n",
       "\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeErrorInternal$(ExecutionErrors.scala:106)\n",
       "\tat org.apache.spark.sql.errors.ExecutionErrors$.invalidInputInCastToDatetimeErrorInternal(ExecutionErrors.scala:268)\n",
       "\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeError(ExecutionErrors.scala:96)\n",
       "\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeError$(ExecutionErrors.scala:92)\n",
       "\tat org.apache.spark.sql.errors.ExecutionErrors$.invalidInputInCastToDatetimeError(ExecutionErrors.scala:268)\n",
       "\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.$anonfun$stringToDateAnsi$1(SparkDateTimeUtils.scala:464)\n",
       "\tat scala.Option.getOrElse(Option.scala:201)\n",
       "\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToDateAnsi(SparkDateTimeUtils.scala:464)\n",
       "\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToDateAnsi$(SparkDateTimeUtils.scala:462)\n",
       "\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils$.stringToDateAnsi(SparkDateTimeUtils.scala:830)\n",
       "\tat org.apache.spark.sql.catalyst.util.DefaultDateFormatter.parse(DateFormatter.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$11$1.applyOrElse(JacksonParser.scala:404)\n",
       "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$11$1.applyOrElse(JacksonParser.scala:401)\n",
       "\tat org.apache.spark.sql.catalyst.json.JacksonParser.doParseJsonToken(JacksonParser.scala:523)\n",
       "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:508)\n",
       "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$11(JacksonParser.scala:401)\n",
       "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:597)\n",
       "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:172)\n",
       "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.json.JacksonParser.doParseJsonToken(JacksonParser.scala:523)\n",
       "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:508)\n",
       "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:722)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithResource(SparkErrorUtils.scala:51)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithResource$(SparkErrorUtils.scala:48)\n",
       "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:110)\n",
       "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:717)\n",
       "\tat com.databricks.photon.NativeJsonReaderHelper$.$anonfun$readSingleLine$1(NativeJsonReaderHelper.scala:99)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n",
       "\tat com.databricks.photon.NativeJsonReaderHelper$.$anonfun$readSingleLine$3(NativeJsonReaderHelper.scala:108)\n",
       "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n",
       "\tat scala.collection.convert.JavaCollectionWrappers$IteratorWrapper.hasNext(JavaCollectionWrappers.scala:32)\n",
       "\tat com.databricks.photon.NativeReader.next(NativeReader.java:108)\n",
       "\tat com.databricks.photon.JniApiImpl.getNext(JniApi.scala:-2)\n",
       "\tat com.databricks.photon.JniApi.getNext(JniApi.scala:-1)\n",
       "\tat com.databricks.photon.JniExecNode.next(JniExecNode.java:84)\n",
       "\tat com.databricks.photon.PhotonColumnarBatchResultHandler$$anon$2.next(PhotonExec.scala:1508)\n",
       "\tat com.databricks.photon.PhotonColumnarBatchResultHandler$$anon$2.next(PhotonExec.scala:1504)\n",
       "\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$next$2(PhotonBasicEvaluatorFactory.scala:293)\n",
       "\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n",
       "\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n",
       "\tat com.databricks.photon.PhotonColumnarBatchResultHandler.timeit(PhotonExec.scala:1492)\n",
       "\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$next$1(PhotonBasicEvaluatorFactory.scala:293)\n",
       "\tat com.databricks.photon.metrics.BillableTimeTaskMetrics.withPhotonBilling(BillableTimeTaskMetrics.scala:71)\n",
       "\tat org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:273)\n",
       "\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.next(PhotonBasicEvaluatorFactory.scala:291)\n",
       "\tat com.databricks.photon.CloseableIterator$$anon$10.next(CloseableIterator.scala:212)\n",
       "\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n",
       "\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:1240)\n",
       "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:636)\n",
       "\tat com.databricks.sql.execution.arrowcollect.RowBasedConverters$$anon$1.<init>(RowBasedConverters.scala:110)\n",
       "\tat com.databricks.sql.execution.arrowcollect.RowBasedConverters$.toHybridResultIterator(RowBasedConverters.scala:99)\n",
       "\tat com.databricks.sql.execution.arrowcollect.HybridJsonArrayCollector$.convertRows(HybridJsonArrayCollector.scala:66)\n",
       "\tat com.databricks.sql.execution.arrowcollect.HybridJsonArrayCollector$.$anonfun$executorCollect$1(HybridJsonArrayCollector.scala:135)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:954)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:954)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:62)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:429)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:426)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:393)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n",
       "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:233)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:197)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:160)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.scheduler.TaskExecutionUtils$.withUCHandleForTaskExecution(Task.scala:355)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$3(Task.scala:154)\n",
       "\tat org.apache.spark.scheduler.TaskExecutionUtils$.withCredentialsForTaskExecution(Task.scala:333)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:113)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1255)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1259)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1108)\n",
       "\tat com.databricks.aether.RDDTask.run(RDDTask.scala:252)\n",
       "\tat com.databricks.aether.worker.WorkerTaskAttemptThread.$anonfun$runInternal$1(AetherWorkerImpl.scala:362)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.aether.AetherInt64Gauge.scopedAdd(AetherServiceMetricImpls.scala:129)\n",
       "\tat com.databricks.aether.worker.WorkerTaskAttemptThread.runInternal(AetherWorkerImpl.scala:351)\n",
       "\tat com.databricks.aether.worker.WorkerTaskAttemptThread.run(AetherWorkerImpl.scala:328)\n",
       "\tat com.databricks.aether.FairBlockingQueue$SlotCountingRunnable.run(FairBlockingQueue.java:638)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SparkException",
        "evalue": "[MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [null,DL,1451,BOS,Boston, MA,ATL,Atlanta, GA,1113,1348,1115,1400,946,null,5,1343].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.  SQLSTATE: 22023\n\nJVM stacktrace:\norg.apache.spark.SparkException\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:2162)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.throwMalformedRecordsDetectedInRecordParsingError(FailureSafeParser.scala:131)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:122)\n\tat com.databricks.photon.NativeJsonReaderHelper$.$anonfun$readSingleLine$3(NativeJsonReaderHelper.scala:108)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n\tat scala.collection.convert.JavaCollectionWrappers$IteratorWrapper.hasNext(JavaCollectionWrappers.scala:32)\n\tat com.databricks.photon.NativeReader.next(NativeReader.java:108)\n\tat 0xc331ca7 <photon>.Next(external/workspace_spark_4_0/photon/jni-wrappers/jni-reader.cc:110)\n\tat 0x7598e8b <photon>.ReaderBatch(external/workspace_spark_4_0/photon/exec-nodes/common-file-scan-node.h:224)\n\tat 0x7598833 <photon>.NextImpl(external/workspace_spark_4_0/photon/exec-nodes/json-file-scan-node.cc:140)\n\tat 0x7494007 <photon>.Next(external/workspace_spark_4_0/photon/exec-nodes/exec-node.cc:222)\n\tat com.databricks.photon.JniApiImpl.getNext(JniApi.scala:-2)\n\tat com.databricks.photon.JniApi.getNext(JniApi.scala:-1)\n\tat com.databricks.photon.JniExecNode.next(JniExecNode.java:84)\n\tat com.databricks.photon.PhotonColumnarBatchResultHandler$$anon$2.next(PhotonExec.scala:1508)\n\tat com.databricks.photon.PhotonColumnarBatchResultHandler$$anon$2.next(PhotonExec.scala:1504)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$next$2(PhotonBasicEvaluatorFactory.scala:293)\n\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n\tat com.databricks.photon.PhotonColumnarBatchResultHandler.timeit(PhotonExec.scala:1492)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$next$1(PhotonBasicEvaluatorFactory.scala:293)\n\tat com.databricks.photon.metrics.BillableTimeTaskMetrics.withPhotonBilling(BillableTimeTaskMetrics.scala:71)\n\tat org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:273)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.next(PhotonBasicEvaluatorFactory.scala:291)\n\tat com.databricks.photon.CloseableIterator$$anon$10.next(CloseableIterator.scala:212)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:1240)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:636)\n\tat com.databricks.sql.execution.arrowcollect.RowBasedConverters$$anon$1.<init>(RowBasedConverters.scala:110)\n\tat com.databricks.sql.execution.arrowcollect.RowBasedConverters$.toHybridResultIterator(RowBasedConverters.scala:99)\n\tat com.databricks.sql.execution.arrowcollect.HybridJsonArrayCollector$.convertRows(HybridJsonArrayCollector.scala:66)\n\tat com.databricks.sql.execution.arrowcollect.HybridJsonArrayCollector$.$anonfun$executorCollect$1(HybridJsonArrayCollector.scala:135)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:954)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:954)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:62)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:429)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:426)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:393)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:233)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:197)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:160)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.TaskExecutionUtils$.withUCHandleForTaskExecution(Task.scala:355)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$3(Task.scala:154)\n\tat org.apache.spark.scheduler.TaskExecutionUtils$.withCredentialsForTaskExecution(Task.scala:333)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:113)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1255)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1259)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1108)\n\tat com.databricks.aether.RDDTask.run(RDDTask.scala:252)\n\tat com.databricks.aether.worker.WorkerTaskAttemptThread.$anonfun$runInternal$1(AetherWorkerImpl.scala:362)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.aether.AetherInt64Gauge.scopedAdd(AetherServiceMetricImpls.scala:129)\n\tat com.databricks.aether.worker.WorkerTaskAttemptThread.runInternal(AetherWorkerImpl.scala:351)\n\tat com.databricks.aether.worker.WorkerTaskAttemptThread.run(AetherWorkerImpl.scala:328)\n\tat com.databricks.aether.FairBlockingQueue$SlotCountingRunnable.run(FairBlockingQueue.java:638)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1709)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1693)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3288)\n\tat com.databricks.sql.execution.arrowcollect.RDDBatchCollector.$anonfun$runSparkJobs$2(RDDBatchCollector.scala:264)\n\tat com.databricks.sql.execution.arrowcollect.RDDBatchCollector.$anonfun$runSparkJobs$2$adapted(RDDBatchCollector.scala:261)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1306)\n\tat com.databricks.sql.execution.arrowcollect.RDDBatchCollector.runSparkJobs(RDDBatchCollector.scala:261)\n\tat com.databricks.sql.execution.arrowcollect.RDDBatchCollector.collect(RDDBatchCollector.scala:347)\n\tat com.databricks.sql.execution.arrowcollect.CloudStoreCollector$.hybridCollect(CloudStoreCollector.scala:159)\n\tat com.databricks.sql.execution.arrowcollect.CloudStoreCollector$.hybridCollect(CloudStoreCollector.scala:206)\n\tat org.apache.spark.sql.execution.qrc.HybridCloudStoreFormat.collect(cachedSparkResults.scala:149)\n\tat org.apache.spark.sql.execution.qrc.HybridCloudStoreFormat.collect(cachedSparkResults.scala:139)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:607)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:624)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:448)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeHybridCloudStoreResult(ResultCacheManager.scala:407)\n\tat org.apache.spark.sql.execution.SparkPlan.executeHybridCloudStoreCollectResult(SparkPlan.scala:631)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$collectToHybridCloudStoreResults$1(Dataset.scala:1837)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$4(Dataset.scala:2766)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$3(Dataset.scala:2764)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2764)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2763)\n\tat org.apache.spark.sql.classic.Dataset.collectToHybridCloudStoreResults(Dataset.scala:1836)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.processAsRemoteBatches(SparkConnectPlanExecution.scala:570)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:149)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:376)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)\nCaused by: org.apache.spark.SparkDateTimeException: [CAST_INVALID_INPUT] The value '1/1/2000' of the type \"STRING\" cannot be cast to \"DATE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeErrorInternal(ExecutionErrors.scala:119)\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeErrorInternal$(ExecutionErrors.scala:106)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.invalidInputInCastToDatetimeErrorInternal(ExecutionErrors.scala:268)\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeError(ExecutionErrors.scala:96)\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeError$(ExecutionErrors.scala:92)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.invalidInputInCastToDatetimeError(ExecutionErrors.scala:268)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.$anonfun$stringToDateAnsi$1(SparkDateTimeUtils.scala:464)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToDateAnsi(SparkDateTimeUtils.scala:464)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToDateAnsi$(SparkDateTimeUtils.scala:462)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils$.stringToDateAnsi(SparkDateTimeUtils.scala:830)\n\tat org.apache.spark.sql.catalyst.util.DefaultDateFormatter.parse(DateFormatter.scala:105)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$11$1.applyOrElse(JacksonParser.scala:404)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$11$1.applyOrElse(JacksonParser.scala:401)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.doParseJsonToken(JacksonParser.scala:523)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:508)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$11(JacksonParser.scala:401)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:597)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:172)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:171)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.doParseJsonToken(JacksonParser.scala:523)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:508)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:171)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:722)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithResource(SparkErrorUtils.scala:51)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithResource$(SparkErrorUtils.scala:48)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:110)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:717)\n\tat com.databricks.photon.NativeJsonReaderHelper$.$anonfun$readSingleLine$1(NativeJsonReaderHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\tat com.databricks.photon.NativeJsonReaderHelper$.$anonfun$readSingleLine$3(NativeJsonReaderHelper.scala:108)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n\tat scala.collection.convert.JavaCollectionWrappers$IteratorWrapper.hasNext(JavaCollectionWrappers.scala:32)\n\tat com.databricks.photon.NativeReader.next(NativeReader.java:108)\n\tat com.databricks.photon.JniApiImpl.getNext(JniApi.scala:-2)\n\tat com.databricks.photon.JniApi.getNext(JniApi.scala:-1)\n\tat com.databricks.photon.JniExecNode.next(JniExecNode.java:84)\n\tat com.databricks.photon.PhotonColumnarBatchResultHandler$$anon$2.next(PhotonExec.scala:1508)\n\tat com.databricks.photon.PhotonColumnarBatchResultHandler$$anon$2.next(PhotonExec.scala:1504)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$next$2(PhotonBasicEvaluatorFactory.scala:293)\n\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n\tat com.databricks.photon.PhotonColumnarBatchResultHandler.timeit(PhotonExec.scala:1492)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$next$1(PhotonBasicEvaluatorFactory.scala:293)\n\tat com.databricks.photon.metrics.BillableTimeTaskMetrics.withPhotonBilling(BillableTimeTaskMetrics.scala:71)\n\tat org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:273)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.next(PhotonBasicEvaluatorFactory.scala:291)\n\tat com.databricks.photon.CloseableIterator$$anon$10.next(CloseableIterator.scala:212)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:1240)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:636)\n\tat com.databricks.sql.execution.arrowcollect.RowBasedConverters$$anon$1.<init>(RowBasedConverters.scala:110)\n\tat com.databricks.sql.execution.arrowcollect.RowBasedConverters$.toHybridResultIterator(RowBasedConverters.scala:99)\n\tat com.databricks.sql.execution.arrowcollect.HybridJsonArrayCollector$.convertRows(HybridJsonArrayCollector.scala:66)\n\tat com.databricks.sql.execution.arrowcollect.HybridJsonArrayCollector$.$anonfun$executorCollect$1(HybridJsonArrayCollector.scala:135)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:954)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:954)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:62)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:429)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:426)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:393)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:233)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:197)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:160)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.TaskExecutionUtils$.withUCHandleForTaskExecution(Task.scala:355)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$3(Task.scala:154)\n\tat org.apache.spark.scheduler.TaskExecutionUtils$.withCredentialsForTaskExecution(Task.scala:333)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:113)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1255)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1259)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1108)\n\tat com.databricks.aether.RDDTask.run(RDDTask.scala:252)\n\tat com.databricks.aether.worker.WorkerTaskAttemptThread.$anonfun$runInternal$1(AetherWorkerImpl.scala:362)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.aether.AetherInt64Gauge.scopedAdd(AetherServiceMetricImpls.scala:129)\n\tat com.databricks.aether.worker.WorkerTaskAttemptThread.runInternal(AetherWorkerImpl.scala:351)\n\tat com.databricks.aether.worker.WorkerTaskAttemptThread.run(AetherWorkerImpl.scala:328)\n\tat com.databricks.aether.FairBlockingQueue$SlotCountingRunnable.run(FairBlockingQueue.java:638)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       },
       "metadata": {
        "errorSummary": "[MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [null,DL,1451,BOS,Boston, MA,ATL,Atlanta, GA,1113,1348,1115,1400,946,null,5,1343].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.  SQLSTATE: 22023"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "22023",
        "stackTrace": "org.apache.spark.SparkException\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:2162)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.throwMalformedRecordsDetectedInRecordParsingError(FailureSafeParser.scala:131)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:122)\n\tat com.databricks.photon.NativeJsonReaderHelper$.$anonfun$readSingleLine$3(NativeJsonReaderHelper.scala:108)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n\tat scala.collection.convert.JavaCollectionWrappers$IteratorWrapper.hasNext(JavaCollectionWrappers.scala:32)\n\tat com.databricks.photon.NativeReader.next(NativeReader.java:108)\n\tat 0xc331ca7 <photon>.Next(external/workspace_spark_4_0/photon/jni-wrappers/jni-reader.cc:110)\n\tat 0x7598e8b <photon>.ReaderBatch(external/workspace_spark_4_0/photon/exec-nodes/common-file-scan-node.h:224)\n\tat 0x7598833 <photon>.NextImpl(external/workspace_spark_4_0/photon/exec-nodes/json-file-scan-node.cc:140)\n\tat 0x7494007 <photon>.Next(external/workspace_spark_4_0/photon/exec-nodes/exec-node.cc:222)\n\tat com.databricks.photon.JniApiImpl.getNext(JniApi.scala:-2)\n\tat com.databricks.photon.JniApi.getNext(JniApi.scala:-1)\n\tat com.databricks.photon.JniExecNode.next(JniExecNode.java:84)\n\tat com.databricks.photon.PhotonColumnarBatchResultHandler$$anon$2.next(PhotonExec.scala:1508)\n\tat com.databricks.photon.PhotonColumnarBatchResultHandler$$anon$2.next(PhotonExec.scala:1504)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$next$2(PhotonBasicEvaluatorFactory.scala:293)\n\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n\tat com.databricks.photon.PhotonColumnarBatchResultHandler.timeit(PhotonExec.scala:1492)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$next$1(PhotonBasicEvaluatorFactory.scala:293)\n\tat com.databricks.photon.metrics.BillableTimeTaskMetrics.withPhotonBilling(BillableTimeTaskMetrics.scala:71)\n\tat org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:273)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.next(PhotonBasicEvaluatorFactory.scala:291)\n\tat com.databricks.photon.CloseableIterator$$anon$10.next(CloseableIterator.scala:212)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:1240)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:636)\n\tat com.databricks.sql.execution.arrowcollect.RowBasedConverters$$anon$1.<init>(RowBasedConverters.scala:110)\n\tat com.databricks.sql.execution.arrowcollect.RowBasedConverters$.toHybridResultIterator(RowBasedConverters.scala:99)\n\tat com.databricks.sql.execution.arrowcollect.HybridJsonArrayCollector$.convertRows(HybridJsonArrayCollector.scala:66)\n\tat com.databricks.sql.execution.arrowcollect.HybridJsonArrayCollector$.$anonfun$executorCollect$1(HybridJsonArrayCollector.scala:135)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:954)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:954)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:62)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:429)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:426)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:393)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:233)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:197)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:160)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.TaskExecutionUtils$.withUCHandleForTaskExecution(Task.scala:355)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$3(Task.scala:154)\n\tat org.apache.spark.scheduler.TaskExecutionUtils$.withCredentialsForTaskExecution(Task.scala:333)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:113)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1255)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1259)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1108)\n\tat com.databricks.aether.RDDTask.run(RDDTask.scala:252)\n\tat com.databricks.aether.worker.WorkerTaskAttemptThread.$anonfun$runInternal$1(AetherWorkerImpl.scala:362)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.aether.AetherInt64Gauge.scopedAdd(AetherServiceMetricImpls.scala:129)\n\tat com.databricks.aether.worker.WorkerTaskAttemptThread.runInternal(AetherWorkerImpl.scala:351)\n\tat com.databricks.aether.worker.WorkerTaskAttemptThread.run(AetherWorkerImpl.scala:328)\n\tat com.databricks.aether.FairBlockingQueue$SlotCountingRunnable.run(FairBlockingQueue.java:638)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1709)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1693)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3288)\n\tat com.databricks.sql.execution.arrowcollect.RDDBatchCollector.$anonfun$runSparkJobs$2(RDDBatchCollector.scala:264)\n\tat com.databricks.sql.execution.arrowcollect.RDDBatchCollector.$anonfun$runSparkJobs$2$adapted(RDDBatchCollector.scala:261)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1306)\n\tat com.databricks.sql.execution.arrowcollect.RDDBatchCollector.runSparkJobs(RDDBatchCollector.scala:261)\n\tat com.databricks.sql.execution.arrowcollect.RDDBatchCollector.collect(RDDBatchCollector.scala:347)\n\tat com.databricks.sql.execution.arrowcollect.CloudStoreCollector$.hybridCollect(CloudStoreCollector.scala:159)\n\tat com.databricks.sql.execution.arrowcollect.CloudStoreCollector$.hybridCollect(CloudStoreCollector.scala:206)\n\tat org.apache.spark.sql.execution.qrc.HybridCloudStoreFormat.collect(cachedSparkResults.scala:149)\n\tat org.apache.spark.sql.execution.qrc.HybridCloudStoreFormat.collect(cachedSparkResults.scala:139)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:607)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:624)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:448)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeHybridCloudStoreResult(ResultCacheManager.scala:407)\n\tat org.apache.spark.sql.execution.SparkPlan.executeHybridCloudStoreCollectResult(SparkPlan.scala:631)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$collectToHybridCloudStoreResults$1(Dataset.scala:1837)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$4(Dataset.scala:2766)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$3(Dataset.scala:2764)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2764)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2763)\n\tat org.apache.spark.sql.classic.Dataset.collectToHybridCloudStoreResults(Dataset.scala:1836)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.processAsRemoteBatches(SparkConnectPlanExecution.scala:570)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:149)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:376)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)\nCaused by: org.apache.spark.SparkDateTimeException: [CAST_INVALID_INPUT] The value '1/1/2000' of the type \"STRING\" cannot be cast to \"DATE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeErrorInternal(ExecutionErrors.scala:119)\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeErrorInternal$(ExecutionErrors.scala:106)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.invalidInputInCastToDatetimeErrorInternal(ExecutionErrors.scala:268)\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeError(ExecutionErrors.scala:96)\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeError$(ExecutionErrors.scala:92)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.invalidInputInCastToDatetimeError(ExecutionErrors.scala:268)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.$anonfun$stringToDateAnsi$1(SparkDateTimeUtils.scala:464)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToDateAnsi(SparkDateTimeUtils.scala:464)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToDateAnsi$(SparkDateTimeUtils.scala:462)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils$.stringToDateAnsi(SparkDateTimeUtils.scala:830)\n\tat org.apache.spark.sql.catalyst.util.DefaultDateFormatter.parse(DateFormatter.scala:105)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$11$1.applyOrElse(JacksonParser.scala:404)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$11$1.applyOrElse(JacksonParser.scala:401)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.doParseJsonToken(JacksonParser.scala:523)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:508)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$11(JacksonParser.scala:401)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:597)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:172)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:171)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.doParseJsonToken(JacksonParser.scala:523)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:508)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:171)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:722)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithResource(SparkErrorUtils.scala:51)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithResource$(SparkErrorUtils.scala:48)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:110)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:717)\n\tat com.databricks.photon.NativeJsonReaderHelper$.$anonfun$readSingleLine$1(NativeJsonReaderHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\tat com.databricks.photon.NativeJsonReaderHelper$.$anonfun$readSingleLine$3(NativeJsonReaderHelper.scala:108)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n\tat scala.collection.convert.JavaCollectionWrappers$IteratorWrapper.hasNext(JavaCollectionWrappers.scala:32)\n\tat com.databricks.photon.NativeReader.next(NativeReader.java:108)\n\tat com.databricks.photon.JniApiImpl.getNext(JniApi.scala:-2)\n\tat com.databricks.photon.JniApi.getNext(JniApi.scala:-1)\n\tat com.databricks.photon.JniExecNode.next(JniExecNode.java:84)\n\tat com.databricks.photon.PhotonColumnarBatchResultHandler$$anon$2.next(PhotonExec.scala:1508)\n\tat com.databricks.photon.PhotonColumnarBatchResultHandler$$anon$2.next(PhotonExec.scala:1504)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$next$2(PhotonBasicEvaluatorFactory.scala:293)\n\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n\tat com.databricks.photon.PhotonColumnarBatchResultHandler.timeit(PhotonExec.scala:1492)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$next$1(PhotonBasicEvaluatorFactory.scala:293)\n\tat com.databricks.photon.metrics.BillableTimeTaskMetrics.withPhotonBilling(BillableTimeTaskMetrics.scala:71)\n\tat org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:273)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.next(PhotonBasicEvaluatorFactory.scala:291)\n\tat com.databricks.photon.CloseableIterator$$anon$10.next(CloseableIterator.scala:212)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:1240)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:636)\n\tat com.databricks.sql.execution.arrowcollect.RowBasedConverters$$anon$1.<init>(RowBasedConverters.scala:110)\n\tat com.databricks.sql.execution.arrowcollect.RowBasedConverters$.toHybridResultIterator(RowBasedConverters.scala:99)\n\tat com.databricks.sql.execution.arrowcollect.HybridJsonArrayCollector$.convertRows(HybridJsonArrayCollector.scala:66)\n\tat com.databricks.sql.execution.arrowcollect.HybridJsonArrayCollector$.$anonfun$executorCollect$1(HybridJsonArrayCollector.scala:135)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:954)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:954)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:62)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:429)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:426)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:393)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:233)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:197)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:160)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.TaskExecutionUtils$.withUCHandleForTaskExecution(Task.scala:355)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$3(Task.scala:154)\n\tat org.apache.spark.scheduler.TaskExecutionUtils$.withCredentialsForTaskExecution(Task.scala:333)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:113)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1255)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1259)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1108)\n\tat com.databricks.aether.RDDTask.run(RDDTask.scala:252)\n\tat com.databricks.aether.worker.WorkerTaskAttemptThread.$anonfun$runInternal$1(AetherWorkerImpl.scala:362)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.aether.AetherInt64Gauge.scopedAdd(AetherServiceMetricImpls.scala:129)\n\tat com.databricks.aether.worker.WorkerTaskAttemptThread.runInternal(AetherWorkerImpl.scala:351)\n\tat com.databricks.aether.worker.WorkerTaskAttemptThread.run(AetherWorkerImpl.scala:328)\n\tat com.databricks.aether.FairBlockingQueue$SlotCountingRunnable.run(FairBlockingQueue.java:638)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mSparkException\u001B[0m                            Traceback (most recent call last)",
        "File \u001B[0;32m<command-4855986274339412>, line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m flight_raw_df1 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\\\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mschema(flight_schema)\\\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFAILFAST\u001B[39m\u001B[38;5;124m\"\u001B[39m)\\\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Volumes/dev/multi_datasets/spark_data/flight-time.json\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 6\u001B[0m display(flight_raw_df1\u001B[38;5;241m.\u001B[39mlimit(\u001B[38;5;241m10\u001B[39m))\n\u001B[1;32m      7\u001B[0m flight_raw_df1\u001B[38;5;241m.\u001B[39mprintSchema()\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:133\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cf_helper \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay_connect_table(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39misStreaming:\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:97\u001B[0m, in \u001B[0;36mDisplay.display_connect_table\u001B[0;34m(self, df, **kwargs)\u001B[0m\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_streaming_dataframe(df, config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstreaming_listener,\n\u001B[1;32m     95\u001B[0m                                                \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 97\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_dataframe(df, config)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:48\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.display_dataframe\u001B[0;34m(self, df, config)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdisplay_dataframe\u001B[39m(\u001B[38;5;28mself\u001B[39m, df: ConnectDataFrame, config: TableDisplayConfig):\n\u001B[0;32m---> 48\u001B[0m     display_payload \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite_to_cloudfetch(df, config)\n\u001B[1;32m     49\u001B[0m     ip_display({\n\u001B[1;32m     50\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/vnd.databricks.connect.display\u001B[39m\u001B[38;5;124m\"\u001B[39m: display_payload,\n\u001B[1;32m     51\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext/plain\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatabricks Spark Connect Table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     52\u001B[0m     },\n\u001B[1;32m     53\u001B[0m                raw\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:134\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n\u001B[1;32m    132\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m PySparkException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 134\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    136\u001B[0m     error_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtype\u001B[39m(\n\u001B[1;32m    137\u001B[0m         e\n\u001B[1;32m    138\u001B[0m     )(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEncountered an unexpected error when displaying table. Try again later, or detaching and reattaching to your compute.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    139\u001B[0m       )\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:103\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m rowLimit \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    101\u001B[0m     connectDataFrame \u001B[38;5;241m=\u001B[39m cast(ConnectDataFrame, connectDataFrame\u001B[38;5;241m.\u001B[39mlimit(rowLimit \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m    102\u001B[0m results: Tuple[Optional[StructType], List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCloudFetchResult\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m--> 103\u001B[0m                List[\u001B[38;5;28mbool\u001B[39m]] \u001B[38;5;241m=\u001B[39m connectDataFrame\u001B[38;5;241m.\u001B[39m_to_cloudfetch_with_limits_and_file_paths(\n\u001B[1;32m    104\u001B[0m                    \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39mstr_format,\n\u001B[1;32m    105\u001B[0m                    compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    106\u001B[0m                    row_limit\u001B[38;5;241m=\u001B[39mrowLimit,\n\u001B[1;32m    107\u001B[0m                    byte_limit\u001B[38;5;241m=\u001B[39mbyteLimit)\n\u001B[1;32m    108\u001B[0m pyspark_struct, cloudfetch_results, batch_truncation_results \u001B[38;5;241m=\u001B[39m results\n\u001B[1;32m    110\u001B[0m schema \u001B[38;5;241m=\u001B[39m CloudFetchDisplayHelper\u001B[38;5;241m.\u001B[39mget_display_schema_from_pyspark_struct(pyspark_struct)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1882\u001B[0m, in \u001B[0;36mDataFrame._to_cloudfetch_with_limits_and_file_paths\u001B[0;34m(self, format, compression, row_limit, byte_limit)\u001B[0m\n\u001B[1;32m   1861\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1862\u001B[0m \u001B[38;5;124;03mThis is an experimental method to support generating results in the form of pre-signed\u001B[39;00m\n\u001B[1;32m   1863\u001B[0m \u001B[38;5;124;03mURLs that can be used by the client to fetch the results directly from cloud storage.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1879\u001B[0m \n\u001B[1;32m   1880\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1881\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 1882\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexperimental_to_cloudfetch(\n\u001B[1;32m   1883\u001B[0m     query, \u001B[38;5;28mformat\u001B[39m, compression, row_limit, byte_limit, \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1884\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1308\u001B[0m, in \u001B[0;36mSparkConnectClient.experimental_to_cloudfetch\u001B[0;34m(self, plan, format, compression, row_limit, byte_limit, with_internal_file_path)\u001B[0m\n\u001B[1;32m   1305\u001B[0m \u001B[38;5;66;03m# Execute the request and iterate over the streaming results. We capture the schema\u001B[39;00m\n\u001B[1;32m   1306\u001B[0m \u001B[38;5;66;03m# response and unpack the CloudResultBatch responses.\u001B[39;00m\n\u001B[1;32m   1307\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1308\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(req, {}, [], progress\u001B[38;5;241m=\u001B[39mprogress):\n\u001B[1;32m   1309\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1310\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mSparkException\u001B[0m: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [null,DL,1451,BOS,Boston, MA,ATL,Atlanta, GA,1113,1348,1115,1400,946,null,5,1343].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.  SQLSTATE: 22023\n\nJVM stacktrace:\norg.apache.spark.SparkException\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:2162)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.throwMalformedRecordsDetectedInRecordParsingError(FailureSafeParser.scala:131)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:122)\n\tat com.databricks.photon.NativeJsonReaderHelper$.$anonfun$readSingleLine$3(NativeJsonReaderHelper.scala:108)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n\tat scala.collection.convert.JavaCollectionWrappers$IteratorWrapper.hasNext(JavaCollectionWrappers.scala:32)\n\tat com.databricks.photon.NativeReader.next(NativeReader.java:108)\n\tat 0xc331ca7 <photon>.Next(external/workspace_spark_4_0/photon/jni-wrappers/jni-reader.cc:110)\n\tat 0x7598e8b <photon>.ReaderBatch(external/workspace_spark_4_0/photon/exec-nodes/common-file-scan-node.h:224)\n\tat 0x7598833 <photon>.NextImpl(external/workspace_spark_4_0/photon/exec-nodes/json-file-scan-node.cc:140)\n\tat 0x7494007 <photon>.Next(external/workspace_spark_4_0/photon/exec-nodes/exec-node.cc:222)\n\tat com.databricks.photon.JniApiImpl.getNext(JniApi.scala:-2)\n\tat com.databricks.photon.JniApi.getNext(JniApi.scala:-1)\n\tat com.databricks.photon.JniExecNode.next(JniExecNode.java:84)\n\tat com.databricks.photon.PhotonColumnarBatchResultHandler$$anon$2.next(PhotonExec.scala:1508)\n\tat com.databricks.photon.PhotonColumnarBatchResultHandler$$anon$2.next(PhotonExec.scala:1504)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$next$2(PhotonBasicEvaluatorFactory.scala:293)\n\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n\tat com.databricks.photon.PhotonColumnarBatchResultHandler.timeit(PhotonExec.scala:1492)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$next$1(PhotonBasicEvaluatorFactory.scala:293)\n\tat com.databricks.photon.metrics.BillableTimeTaskMetrics.withPhotonBilling(BillableTimeTaskMetrics.scala:71)\n\tat org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:273)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.next(PhotonBasicEvaluatorFactory.scala:291)\n\tat com.databricks.photon.CloseableIterator$$anon$10.next(CloseableIterator.scala:212)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:1240)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:636)\n\tat com.databricks.sql.execution.arrowcollect.RowBasedConverters$$anon$1.<init>(RowBasedConverters.scala:110)\n\tat com.databricks.sql.execution.arrowcollect.RowBasedConverters$.toHybridResultIterator(RowBasedConverters.scala:99)\n\tat com.databricks.sql.execution.arrowcollect.HybridJsonArrayCollector$.convertRows(HybridJsonArrayCollector.scala:66)\n\tat com.databricks.sql.execution.arrowcollect.HybridJsonArrayCollector$.$anonfun$executorCollect$1(HybridJsonArrayCollector.scala:135)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:954)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:954)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:62)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:429)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:426)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:393)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:233)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:197)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:160)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.TaskExecutionUtils$.withUCHandleForTaskExecution(Task.scala:355)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$3(Task.scala:154)\n\tat org.apache.spark.scheduler.TaskExecutionUtils$.withCredentialsForTaskExecution(Task.scala:333)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:113)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1255)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1259)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1108)\n\tat com.databricks.aether.RDDTask.run(RDDTask.scala:252)\n\tat com.databricks.aether.worker.WorkerTaskAttemptThread.$anonfun$runInternal$1(AetherWorkerImpl.scala:362)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.aether.AetherInt64Gauge.scopedAdd(AetherServiceMetricImpls.scala:129)\n\tat com.databricks.aether.worker.WorkerTaskAttemptThread.runInternal(AetherWorkerImpl.scala:351)\n\tat com.databricks.aether.worker.WorkerTaskAttemptThread.run(AetherWorkerImpl.scala:328)\n\tat com.databricks.aether.FairBlockingQueue$SlotCountingRunnable.run(FairBlockingQueue.java:638)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1709)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1693)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3288)\n\tat com.databricks.sql.execution.arrowcollect.RDDBatchCollector.$anonfun$runSparkJobs$2(RDDBatchCollector.scala:264)\n\tat com.databricks.sql.execution.arrowcollect.RDDBatchCollector.$anonfun$runSparkJobs$2$adapted(RDDBatchCollector.scala:261)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1306)\n\tat com.databricks.sql.execution.arrowcollect.RDDBatchCollector.runSparkJobs(RDDBatchCollector.scala:261)\n\tat com.databricks.sql.execution.arrowcollect.RDDBatchCollector.collect(RDDBatchCollector.scala:347)\n\tat com.databricks.sql.execution.arrowcollect.CloudStoreCollector$.hybridCollect(CloudStoreCollector.scala:159)\n\tat com.databricks.sql.execution.arrowcollect.CloudStoreCollector$.hybridCollect(CloudStoreCollector.scala:206)\n\tat org.apache.spark.sql.execution.qrc.HybridCloudStoreFormat.collect(cachedSparkResults.scala:149)\n\tat org.apache.spark.sql.execution.qrc.HybridCloudStoreFormat.collect(cachedSparkResults.scala:139)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:607)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:624)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:448)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeHybridCloudStoreResult(ResultCacheManager.scala:407)\n\tat org.apache.spark.sql.execution.SparkPlan.executeHybridCloudStoreCollectResult(SparkPlan.scala:631)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$collectToHybridCloudStoreResults$1(Dataset.scala:1837)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$4(Dataset.scala:2766)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$3(Dataset.scala:2764)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2764)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2763)\n\tat org.apache.spark.sql.classic.Dataset.collectToHybridCloudStoreResults(Dataset.scala:1836)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.processAsRemoteBatches(SparkConnectPlanExecution.scala:570)\n\tat org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:149)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:376)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)\nCaused by: org.apache.spark.SparkDateTimeException: [CAST_INVALID_INPUT] The value '1/1/2000' of the type \"STRING\" cannot be cast to \"DATE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeErrorInternal(ExecutionErrors.scala:119)\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeErrorInternal$(ExecutionErrors.scala:106)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.invalidInputInCastToDatetimeErrorInternal(ExecutionErrors.scala:268)\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeError(ExecutionErrors.scala:96)\n\tat org.apache.spark.sql.errors.ExecutionErrors.invalidInputInCastToDatetimeError$(ExecutionErrors.scala:92)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.invalidInputInCastToDatetimeError(ExecutionErrors.scala:268)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.$anonfun$stringToDateAnsi$1(SparkDateTimeUtils.scala:464)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToDateAnsi(SparkDateTimeUtils.scala:464)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.stringToDateAnsi$(SparkDateTimeUtils.scala:462)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils$.stringToDateAnsi(SparkDateTimeUtils.scala:830)\n\tat org.apache.spark.sql.catalyst.util.DefaultDateFormatter.parse(DateFormatter.scala:105)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$11$1.applyOrElse(JacksonParser.scala:404)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$11$1.applyOrElse(JacksonParser.scala:401)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.doParseJsonToken(JacksonParser.scala:523)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:508)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$11(JacksonParser.scala:401)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:597)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:172)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:171)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.doParseJsonToken(JacksonParser.scala:523)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:508)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:171)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:722)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithResource(SparkErrorUtils.scala:51)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithResource$(SparkErrorUtils.scala:48)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:110)\n\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:717)\n\tat com.databricks.photon.NativeJsonReaderHelper$.$anonfun$readSingleLine$1(NativeJsonReaderHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\tat com.databricks.photon.NativeJsonReaderHelper$.$anonfun$readSingleLine$3(NativeJsonReaderHelper.scala:108)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)\n\tat scala.collection.convert.JavaCollectionWrappers$IteratorWrapper.hasNext(JavaCollectionWrappers.scala:32)\n\tat com.databricks.photon.NativeReader.next(NativeReader.java:108)\n\tat com.databricks.photon.JniApiImpl.getNext(JniApi.scala:-2)\n\tat com.databricks.photon.JniApi.getNext(JniApi.scala:-1)\n\tat com.databricks.photon.JniExecNode.next(JniExecNode.java:84)\n\tat com.databricks.photon.PhotonColumnarBatchResultHandler$$anon$2.next(PhotonExec.scala:1508)\n\tat com.databricks.photon.PhotonColumnarBatchResultHandler$$anon$2.next(PhotonExec.scala:1504)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$next$2(PhotonBasicEvaluatorFactory.scala:293)\n\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n\tat com.databricks.photon.PhotonColumnarBatchResultHandler.timeit(PhotonExec.scala:1492)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$next$1(PhotonBasicEvaluatorFactory.scala:293)\n\tat com.databricks.photon.metrics.BillableTimeTaskMetrics.withPhotonBilling(BillableTimeTaskMetrics.scala:71)\n\tat org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:273)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.next(PhotonBasicEvaluatorFactory.scala:291)\n\tat com.databricks.photon.CloseableIterator$$anon$10.next(CloseableIterator.scala:212)\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:1240)\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:636)\n\tat com.databricks.sql.execution.arrowcollect.RowBasedConverters$$anon$1.<init>(RowBasedConverters.scala:110)\n\tat com.databricks.sql.execution.arrowcollect.RowBasedConverters$.toHybridResultIterator(RowBasedConverters.scala:99)\n\tat com.databricks.sql.execution.arrowcollect.HybridJsonArrayCollector$.convertRows(HybridJsonArrayCollector.scala:66)\n\tat com.databricks.sql.execution.arrowcollect.HybridJsonArrayCollector$.$anonfun$executorCollect$1(HybridJsonArrayCollector.scala:135)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:954)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:954)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:62)\n\tat org.apache.spark.rdd.RDD.$anonfun$computeOrReadCheckpoint$1(RDD.scala:429)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:426)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:393)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:233)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:197)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:160)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.TaskExecutionUtils$.withUCHandleForTaskExecution(Task.scala:355)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$3(Task.scala:154)\n\tat org.apache.spark.scheduler.TaskExecutionUtils$.withCredentialsForTaskExecution(Task.scala:333)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:113)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1255)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1259)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1108)\n\tat com.databricks.aether.RDDTask.run(RDDTask.scala:252)\n\tat com.databricks.aether.worker.WorkerTaskAttemptThread.$anonfun$runInternal$1(AetherWorkerImpl.scala:362)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.aether.AetherInt64Gauge.scopedAdd(AetherServiceMetricImpls.scala:129)\n\tat com.databricks.aether.worker.WorkerTaskAttemptThread.runInternal(AetherWorkerImpl.scala:351)\n\tat com.databricks.aether.worker.WorkerTaskAttemptThread.run(AetherWorkerImpl.scala:328)\n\tat com.databricks.aether.FairBlockingQueue$SlotCountingRunnable.run(FairBlockingQueue.java:638)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "flight_raw_df1 = spark.read\\\n",
    "    .format(\"json\")\\\n",
    "    .schema(flight_schema)\\\n",
    "    .option(\"mode\", \"FAILFAST\")\\\n",
    "    .load(\"/Volumes/dev/multi_datasets/spark_data/flight-time.json\")\n",
    "display(flight_raw_df1.limit(10))\n",
    "flight_raw_df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f24f6b2-01c6-4f33-aff0-705c634dace5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**To process the data we will set option as PERMISSIVE and proceed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5748cea-bc3d-45c3-a83e-99cc5d7e040a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>FL_DATE</th><th>OP_CARRIER</th><th>OP_CARRIER_FL_NUM</th><th>ORIGIN</th><th>ORIGIN_CITY_NAME</th><th>DEST</th><th>DEST_CITY_NAME</th><th>DEP_TIME</th><th>ARR_TIME</th><th>CRS_DEP_TIME</th><th>CRS_ARR_TIME</th><th>DISTANCE</th><th>CANCELLED</th><th>TAXI_IN</th><th>WHEELS_ON</th></tr></thead><tbody><tr><td>null</td><td>DL</td><td>1451</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1113</td><td>1348</td><td>1115</td><td>1400</td><td>946</td><td>null</td><td>5</td><td>1343</td></tr><tr><td>null</td><td>DL</td><td>1479</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1311</td><td>1543</td><td>1315</td><td>1559</td><td>946</td><td>null</td><td>7</td><td>1536</td></tr><tr><td>null</td><td>DL</td><td>1857</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1414</td><td>1651</td><td>1415</td><td>1721</td><td>946</td><td>null</td><td>9</td><td>1642</td></tr><tr><td>null</td><td>DL</td><td>1997</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1720</td><td>2005</td><td>1715</td><td>2013</td><td>946</td><td>null</td><td>10</td><td>1955</td></tr><tr><td>null</td><td>DL</td><td>2065</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>2010</td><td>2240</td><td>2015</td><td>2300</td><td>946</td><td>null</td><td>10</td><td>2230</td></tr><tr><td>null</td><td>US</td><td>2619</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>649</td><td>1003</td><td>650</td><td>955</td><td>946</td><td>null</td><td>7</td><td>956</td></tr><tr><td>null</td><td>US</td><td>2621</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1446</td><td>1717</td><td>1440</td><td>1738</td><td>946</td><td>null</td><td>4</td><td>1713</td></tr><tr><td>null</td><td>DL</td><td>346</td><td>BTR</td><td>Baton Rouge, LA</td><td>ATL</td><td>Atlanta, GA</td><td>1744</td><td>2006</td><td>1740</td><td>2008</td><td>449</td><td>null</td><td>9</td><td>1957</td></tr><tr><td>null</td><td>DL</td><td>412</td><td>BTR</td><td>Baton Rouge, LA</td><td>ATL</td><td>Atlanta, GA</td><td>1345</td><td>1601</td><td>1345</td><td>1622</td><td>449</td><td>null</td><td>9</td><td>1552</td></tr><tr><td>null</td><td>DL</td><td>299</td><td>BUF</td><td>Buffalo, NY</td><td>ATL</td><td>Atlanta, GA</td><td>1245</td><td>1448</td><td>1245</td><td>1455</td><td>712</td><td>null</td><td>5</td><td>1443</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         "DL",
         1451,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1113,
         1348,
         1115,
         1400,
         946,
         null,
         5,
         1343
        ],
        [
         null,
         "DL",
         1479,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1311,
         1543,
         1315,
         1559,
         946,
         null,
         7,
         1536
        ],
        [
         null,
         "DL",
         1857,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1414,
         1651,
         1415,
         1721,
         946,
         null,
         9,
         1642
        ],
        [
         null,
         "DL",
         1997,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1720,
         2005,
         1715,
         2013,
         946,
         null,
         10,
         1955
        ],
        [
         null,
         "DL",
         2065,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         2010,
         2240,
         2015,
         2300,
         946,
         null,
         10,
         2230
        ],
        [
         null,
         "US",
         2619,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         649,
         1003,
         650,
         955,
         946,
         null,
         7,
         956
        ],
        [
         null,
         "US",
         2621,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1446,
         1717,
         1440,
         1738,
         946,
         null,
         4,
         1713
        ],
        [
         null,
         "DL",
         346,
         "BTR",
         "Baton Rouge, LA",
         "ATL",
         "Atlanta, GA",
         1744,
         2006,
         1740,
         2008,
         449,
         null,
         9,
         1957
        ],
        [
         null,
         "DL",
         412,
         "BTR",
         "Baton Rouge, LA",
         "ATL",
         "Atlanta, GA",
         1345,
         1601,
         1345,
         1622,
         449,
         null,
         9,
         1552
        ],
        [
         null,
         "DL",
         299,
         "BUF",
         "Buffalo, NY",
         "ATL",
         "Atlanta, GA",
         1245,
         1448,
         1245,
         1455,
         712,
         null,
         5,
         1443
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "FL_DATE",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "OP_CARRIER",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "OP_CARRIER_FL_NUM",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_CITY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEST",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEST_CITY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEP_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_DEP_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_ARR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DISTANCE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CANCELLED",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "TAXI_IN",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "WHEELS_ON",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- FL_DATE: date (nullable = true)\n |-- OP_CARRIER: string (nullable = true)\n |-- OP_CARRIER_FL_NUM: long (nullable = true)\n |-- ORIGIN: string (nullable = true)\n |-- ORIGIN_CITY_NAME: string (nullable = true)\n |-- DEST: string (nullable = true)\n |-- DEST_CITY_NAME: string (nullable = true)\n |-- DEP_TIME: long (nullable = true)\n |-- ARR_TIME: long (nullable = true)\n |-- CRS_DEP_TIME: long (nullable = true)\n |-- CRS_ARR_TIME: long (nullable = true)\n |-- DISTANCE: long (nullable = true)\n |-- CANCELLED: boolean (nullable = true)\n |-- TAXI_IN: long (nullable = true)\n |-- WHEELS_ON: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "flight_raw_df1 = spark.read\\\n",
    "    .format(\"json\")\\\n",
    "    .schema(flight_schema)\\\n",
    "    .option(\"mode\", \"PERMISSIVE\")\\\n",
    "    .load(\"/Volumes/dev/multi_datasets/spark_data/flight-time.json\")\n",
    "display(flight_raw_df1.limit(10))\n",
    "flight_raw_df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fc40795-0217-4693-9008-14c9a9dda592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**How to solve this issue--- First read FL_DATE and CANCELLED as string and then will change it later after reading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "059b4497-8e4a-4c90-817f-301c508ad486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flight_schema = \"\"\"\n",
    "    FL_DATE string,\n",
    "    OP_CARRIER string,\n",
    "    OP_CARRIER_FL_NUM long,\n",
    "    ORIGIN string,\n",
    "    ORIGIN_CITY_NAME string,\n",
    "    DEST string,\n",
    "    DEST_CITY_NAME string,\n",
    "    DEP_TIME long,\n",
    "    ARR_TIME long,\n",
    "    CRS_DEP_TIME long,\n",
    "    CRS_ARR_TIME long,\n",
    "    DISTANCE long,\n",
    "    CANCELLED string,\n",
    "    TAXI_IN long,\n",
    "    WHEELS_ON long\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e11e1e50-5730-43f1-ad6a-0d10a1528067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>FL_DATE</th><th>OP_CARRIER</th><th>OP_CARRIER_FL_NUM</th><th>ORIGIN</th><th>ORIGIN_CITY_NAME</th><th>DEST</th><th>DEST_CITY_NAME</th><th>DEP_TIME</th><th>ARR_TIME</th><th>CRS_DEP_TIME</th><th>CRS_ARR_TIME</th><th>DISTANCE</th><th>CANCELLED</th><th>TAXI_IN</th><th>WHEELS_ON</th></tr></thead><tbody><tr><td>1/1/2000</td><td>DL</td><td>1451</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1113</td><td>1348</td><td>1115</td><td>1400</td><td>946</td><td>0</td><td>5</td><td>1343</td></tr><tr><td>1/1/2000</td><td>DL</td><td>1479</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1311</td><td>1543</td><td>1315</td><td>1559</td><td>946</td><td>0</td><td>7</td><td>1536</td></tr><tr><td>1/1/2000</td><td>DL</td><td>1857</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1414</td><td>1651</td><td>1415</td><td>1721</td><td>946</td><td>0</td><td>9</td><td>1642</td></tr><tr><td>1/1/2000</td><td>DL</td><td>1997</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1720</td><td>2005</td><td>1715</td><td>2013</td><td>946</td><td>0</td><td>10</td><td>1955</td></tr><tr><td>1/1/2000</td><td>DL</td><td>2065</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>2010</td><td>2240</td><td>2015</td><td>2300</td><td>946</td><td>0</td><td>10</td><td>2230</td></tr><tr><td>1/1/2000</td><td>US</td><td>2619</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>649</td><td>1003</td><td>650</td><td>955</td><td>946</td><td>0</td><td>7</td><td>956</td></tr><tr><td>1/1/2000</td><td>US</td><td>2621</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1446</td><td>1717</td><td>1440</td><td>1738</td><td>946</td><td>0</td><td>4</td><td>1713</td></tr><tr><td>1/1/2000</td><td>DL</td><td>346</td><td>BTR</td><td>Baton Rouge, LA</td><td>ATL</td><td>Atlanta, GA</td><td>1744</td><td>2006</td><td>1740</td><td>2008</td><td>449</td><td>0</td><td>9</td><td>1957</td></tr><tr><td>1/1/2000</td><td>DL</td><td>412</td><td>BTR</td><td>Baton Rouge, LA</td><td>ATL</td><td>Atlanta, GA</td><td>1345</td><td>1601</td><td>1345</td><td>1622</td><td>449</td><td>0</td><td>9</td><td>1552</td></tr><tr><td>1/1/2000</td><td>DL</td><td>299</td><td>BUF</td><td>Buffalo, NY</td><td>ATL</td><td>Atlanta, GA</td><td>1245</td><td>1448</td><td>1245</td><td>1455</td><td>712</td><td>0</td><td>5</td><td>1443</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1/1/2000",
         "DL",
         1451,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1113,
         1348,
         1115,
         1400,
         946,
         "0",
         5,
         1343
        ],
        [
         "1/1/2000",
         "DL",
         1479,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1311,
         1543,
         1315,
         1559,
         946,
         "0",
         7,
         1536
        ],
        [
         "1/1/2000",
         "DL",
         1857,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1414,
         1651,
         1415,
         1721,
         946,
         "0",
         9,
         1642
        ],
        [
         "1/1/2000",
         "DL",
         1997,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1720,
         2005,
         1715,
         2013,
         946,
         "0",
         10,
         1955
        ],
        [
         "1/1/2000",
         "DL",
         2065,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         2010,
         2240,
         2015,
         2300,
         946,
         "0",
         10,
         2230
        ],
        [
         "1/1/2000",
         "US",
         2619,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         649,
         1003,
         650,
         955,
         946,
         "0",
         7,
         956
        ],
        [
         "1/1/2000",
         "US",
         2621,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1446,
         1717,
         1440,
         1738,
         946,
         "0",
         4,
         1713
        ],
        [
         "1/1/2000",
         "DL",
         346,
         "BTR",
         "Baton Rouge, LA",
         "ATL",
         "Atlanta, GA",
         1744,
         2006,
         1740,
         2008,
         449,
         "0",
         9,
         1957
        ],
        [
         "1/1/2000",
         "DL",
         412,
         "BTR",
         "Baton Rouge, LA",
         "ATL",
         "Atlanta, GA",
         1345,
         1601,
         1345,
         1622,
         449,
         "0",
         9,
         1552
        ],
        [
         "1/1/2000",
         "DL",
         299,
         "BUF",
         "Buffalo, NY",
         "ATL",
         "Atlanta, GA",
         1245,
         1448,
         1245,
         1455,
         712,
         "0",
         5,
         1443
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "FL_DATE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "OP_CARRIER",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "OP_CARRIER_FL_NUM",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_CITY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEST",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEST_CITY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEP_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_DEP_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_ARR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DISTANCE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CANCELLED",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TAXI_IN",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "WHEELS_ON",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- FL_DATE: string (nullable = true)\n |-- OP_CARRIER: string (nullable = true)\n |-- OP_CARRIER_FL_NUM: long (nullable = true)\n |-- ORIGIN: string (nullable = true)\n |-- ORIGIN_CITY_NAME: string (nullable = true)\n |-- DEST: string (nullable = true)\n |-- DEST_CITY_NAME: string (nullable = true)\n |-- DEP_TIME: long (nullable = true)\n |-- ARR_TIME: long (nullable = true)\n |-- CRS_DEP_TIME: long (nullable = true)\n |-- CRS_ARR_TIME: long (nullable = true)\n |-- DISTANCE: long (nullable = true)\n |-- CANCELLED: string (nullable = true)\n |-- TAXI_IN: long (nullable = true)\n |-- WHEELS_ON: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "flight_raw_df2 = spark.read\\\n",
    "    .format(\"json\")\\\n",
    "    .schema(flight_schema)\\\n",
    "    .option(\"mode\", \"PERMISSIVE\")\\\n",
    "    .load(\"/Volumes/dev/multi_datasets/spark_data/flight-time.json\")\n",
    "display(flight_raw_df2.limit(10))\n",
    "flight_raw_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1da0bb7-de63-484f-b4ec-4053cdc7af1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, expr\n",
    "flight_raw_df3 = flight_raw_df2\\\n",
    "                    .withColumn(\"FL_DATE\", to_date(\"FL_DATE\", \"M/d/y\"))\\\n",
    "                    .withColumn(\"CANCELLED\", expr(\"if(CANCELLED == '1', true, false)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12dcdf2f-12fe-44d1-b4c3-480cc8bf464f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>FL_DATE</th><th>OP_CARRIER</th><th>OP_CARRIER_FL_NUM</th><th>ORIGIN</th><th>ORIGIN_CITY_NAME</th><th>DEST</th><th>DEST_CITY_NAME</th><th>DEP_TIME</th><th>ARR_TIME</th><th>CRS_DEP_TIME</th><th>CRS_ARR_TIME</th><th>DISTANCE</th><th>CANCELLED</th><th>TAXI_IN</th><th>WHEELS_ON</th></tr></thead><tbody><tr><td>2000-01-01</td><td>DL</td><td>1451</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1113</td><td>1348</td><td>1115</td><td>1400</td><td>946</td><td>false</td><td>5</td><td>1343</td></tr><tr><td>2000-01-01</td><td>DL</td><td>1479</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1311</td><td>1543</td><td>1315</td><td>1559</td><td>946</td><td>false</td><td>7</td><td>1536</td></tr><tr><td>2000-01-01</td><td>DL</td><td>1857</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1414</td><td>1651</td><td>1415</td><td>1721</td><td>946</td><td>false</td><td>9</td><td>1642</td></tr><tr><td>2000-01-01</td><td>DL</td><td>1997</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1720</td><td>2005</td><td>1715</td><td>2013</td><td>946</td><td>false</td><td>10</td><td>1955</td></tr><tr><td>2000-01-01</td><td>DL</td><td>2065</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>2010</td><td>2240</td><td>2015</td><td>2300</td><td>946</td><td>false</td><td>10</td><td>2230</td></tr><tr><td>2000-01-01</td><td>US</td><td>2619</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>649</td><td>1003</td><td>650</td><td>955</td><td>946</td><td>false</td><td>7</td><td>956</td></tr><tr><td>2000-01-01</td><td>US</td><td>2621</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1446</td><td>1717</td><td>1440</td><td>1738</td><td>946</td><td>false</td><td>4</td><td>1713</td></tr><tr><td>2000-01-01</td><td>DL</td><td>346</td><td>BTR</td><td>Baton Rouge, LA</td><td>ATL</td><td>Atlanta, GA</td><td>1744</td><td>2006</td><td>1740</td><td>2008</td><td>449</td><td>false</td><td>9</td><td>1957</td></tr><tr><td>2000-01-01</td><td>DL</td><td>412</td><td>BTR</td><td>Baton Rouge, LA</td><td>ATL</td><td>Atlanta, GA</td><td>1345</td><td>1601</td><td>1345</td><td>1622</td><td>449</td><td>false</td><td>9</td><td>1552</td></tr><tr><td>2000-01-01</td><td>DL</td><td>299</td><td>BUF</td><td>Buffalo, NY</td><td>ATL</td><td>Atlanta, GA</td><td>1245</td><td>1448</td><td>1245</td><td>1455</td><td>712</td><td>false</td><td>5</td><td>1443</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2000-01-01",
         "DL",
         1451,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1113,
         1348,
         1115,
         1400,
         946,
         false,
         5,
         1343
        ],
        [
         "2000-01-01",
         "DL",
         1479,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1311,
         1543,
         1315,
         1559,
         946,
         false,
         7,
         1536
        ],
        [
         "2000-01-01",
         "DL",
         1857,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1414,
         1651,
         1415,
         1721,
         946,
         false,
         9,
         1642
        ],
        [
         "2000-01-01",
         "DL",
         1997,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1720,
         2005,
         1715,
         2013,
         946,
         false,
         10,
         1955
        ],
        [
         "2000-01-01",
         "DL",
         2065,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         2010,
         2240,
         2015,
         2300,
         946,
         false,
         10,
         2230
        ],
        [
         "2000-01-01",
         "US",
         2619,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         649,
         1003,
         650,
         955,
         946,
         false,
         7,
         956
        ],
        [
         "2000-01-01",
         "US",
         2621,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1446,
         1717,
         1440,
         1738,
         946,
         false,
         4,
         1713
        ],
        [
         "2000-01-01",
         "DL",
         346,
         "BTR",
         "Baton Rouge, LA",
         "ATL",
         "Atlanta, GA",
         1744,
         2006,
         1740,
         2008,
         449,
         false,
         9,
         1957
        ],
        [
         "2000-01-01",
         "DL",
         412,
         "BTR",
         "Baton Rouge, LA",
         "ATL",
         "Atlanta, GA",
         1345,
         1601,
         1345,
         1622,
         449,
         false,
         9,
         1552
        ],
        [
         "2000-01-01",
         "DL",
         299,
         "BUF",
         "Buffalo, NY",
         "ATL",
         "Atlanta, GA",
         1245,
         1448,
         1245,
         1455,
         712,
         false,
         5,
         1443
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "FL_DATE",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "OP_CARRIER",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "OP_CARRIER_FL_NUM",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_CITY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEST",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEST_CITY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEP_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_DEP_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_ARR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DISTANCE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CANCELLED",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "TAXI_IN",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "WHEELS_ON",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- FL_DATE: date (nullable = true)\n |-- OP_CARRIER: string (nullable = true)\n |-- OP_CARRIER_FL_NUM: long (nullable = true)\n |-- ORIGIN: string (nullable = true)\n |-- ORIGIN_CITY_NAME: string (nullable = true)\n |-- DEST: string (nullable = true)\n |-- DEST_CITY_NAME: string (nullable = true)\n |-- DEP_TIME: long (nullable = true)\n |-- ARR_TIME: long (nullable = true)\n |-- CRS_DEP_TIME: long (nullable = true)\n |-- CRS_ARR_TIME: long (nullable = true)\n |-- DISTANCE: long (nullable = true)\n |-- CANCELLED: boolean (nullable = false)\n |-- TAXI_IN: long (nullable = true)\n |-- WHEELS_ON: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "display(flight_raw_df3.limit(10))\n",
    "flight_raw_df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "602cb50e-2845-4b5a-bff9-698fce5cc872",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Congrats we solved the issue**\n",
    "\n",
    "**Now the FLDATE is date and loaded correctly also CANCELLED is boolean with TRUE & FALSE value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6357c1ec-531c-4936-9ecc-7809803deb5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flight_raw_df3.write\\\n",
    "    .format(\"delta\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .saveAsTable(\"flight_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a1dedc0-ee84-4574-9a68-e6c9287033fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>FL_DATE</th><th>OP_CARRIER</th><th>OP_CARRIER_FL_NUM</th><th>ORIGIN</th><th>ORIGIN_CITY_NAME</th><th>DEST</th><th>DEST_CITY_NAME</th><th>DEP_TIME</th><th>ARR_TIME</th><th>CRS_DEP_TIME</th><th>CRS_ARR_TIME</th><th>DISTANCE</th><th>CANCELLED</th><th>TAXI_IN</th><th>WHEELS_ON</th></tr></thead><tbody><tr><td>2000-01-01</td><td>DL</td><td>1451</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1113</td><td>1348</td><td>1115</td><td>1400</td><td>946</td><td>false</td><td>5</td><td>1343</td></tr><tr><td>2000-01-01</td><td>DL</td><td>1479</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1311</td><td>1543</td><td>1315</td><td>1559</td><td>946</td><td>false</td><td>7</td><td>1536</td></tr><tr><td>2000-01-01</td><td>DL</td><td>1857</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1414</td><td>1651</td><td>1415</td><td>1721</td><td>946</td><td>false</td><td>9</td><td>1642</td></tr><tr><td>2000-01-01</td><td>DL</td><td>1997</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1720</td><td>2005</td><td>1715</td><td>2013</td><td>946</td><td>false</td><td>10</td><td>1955</td></tr><tr><td>2000-01-01</td><td>DL</td><td>2065</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>2010</td><td>2240</td><td>2015</td><td>2300</td><td>946</td><td>false</td><td>10</td><td>2230</td></tr><tr><td>2000-01-01</td><td>US</td><td>2619</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>649</td><td>1003</td><td>650</td><td>955</td><td>946</td><td>false</td><td>7</td><td>956</td></tr><tr><td>2000-01-01</td><td>US</td><td>2621</td><td>BOS</td><td>Boston, MA</td><td>ATL</td><td>Atlanta, GA</td><td>1446</td><td>1717</td><td>1440</td><td>1738</td><td>946</td><td>false</td><td>4</td><td>1713</td></tr><tr><td>2000-01-01</td><td>DL</td><td>346</td><td>BTR</td><td>Baton Rouge, LA</td><td>ATL</td><td>Atlanta, GA</td><td>1744</td><td>2006</td><td>1740</td><td>2008</td><td>449</td><td>false</td><td>9</td><td>1957</td></tr><tr><td>2000-01-01</td><td>DL</td><td>412</td><td>BTR</td><td>Baton Rouge, LA</td><td>ATL</td><td>Atlanta, GA</td><td>1345</td><td>1601</td><td>1345</td><td>1622</td><td>449</td><td>false</td><td>9</td><td>1552</td></tr><tr><td>2000-01-01</td><td>DL</td><td>299</td><td>BUF</td><td>Buffalo, NY</td><td>ATL</td><td>Atlanta, GA</td><td>1245</td><td>1448</td><td>1245</td><td>1455</td><td>712</td><td>false</td><td>5</td><td>1443</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2000-01-01",
         "DL",
         1451,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1113,
         1348,
         1115,
         1400,
         946,
         false,
         5,
         1343
        ],
        [
         "2000-01-01",
         "DL",
         1479,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1311,
         1543,
         1315,
         1559,
         946,
         false,
         7,
         1536
        ],
        [
         "2000-01-01",
         "DL",
         1857,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1414,
         1651,
         1415,
         1721,
         946,
         false,
         9,
         1642
        ],
        [
         "2000-01-01",
         "DL",
         1997,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1720,
         2005,
         1715,
         2013,
         946,
         false,
         10,
         1955
        ],
        [
         "2000-01-01",
         "DL",
         2065,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         2010,
         2240,
         2015,
         2300,
         946,
         false,
         10,
         2230
        ],
        [
         "2000-01-01",
         "US",
         2619,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         649,
         1003,
         650,
         955,
         946,
         false,
         7,
         956
        ],
        [
         "2000-01-01",
         "US",
         2621,
         "BOS",
         "Boston, MA",
         "ATL",
         "Atlanta, GA",
         1446,
         1717,
         1440,
         1738,
         946,
         false,
         4,
         1713
        ],
        [
         "2000-01-01",
         "DL",
         346,
         "BTR",
         "Baton Rouge, LA",
         "ATL",
         "Atlanta, GA",
         1744,
         2006,
         1740,
         2008,
         449,
         false,
         9,
         1957
        ],
        [
         "2000-01-01",
         "DL",
         412,
         "BTR",
         "Baton Rouge, LA",
         "ATL",
         "Atlanta, GA",
         1345,
         1601,
         1345,
         1622,
         449,
         false,
         9,
         1552
        ],
        [
         "2000-01-01",
         "DL",
         299,
         "BUF",
         "Buffalo, NY",
         "ATL",
         "Atlanta, GA",
         1245,
         1448,
         1245,
         1455,
         712,
         false,
         5,
         1443
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "FL_DATE",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "OP_CARRIER",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "OP_CARRIER_FL_NUM",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "ORIGIN",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "ORIGIN_CITY_NAME",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "DEST",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "DEST_CITY_NAME",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "DEP_TIME",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "ARR_TIME",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "CRS_DEP_TIME",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "CRS_ARR_TIME",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "DISTANCE",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "CANCELLED",
            "nullable": true,
            "type": "boolean"
           },
           {
            "metadata": {},
            "name": "TAXI_IN",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "WHEELS_ON",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 61
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "FL_DATE",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "OP_CARRIER",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "OP_CARRIER_FL_NUM",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORIGIN_CITY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEST",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEST_CITY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DEP_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ARR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_DEP_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CRS_ARR_TIME",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "DISTANCE",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "CANCELLED",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "TAXI_IN",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "WHEELS_ON",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from flight_raw limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e67304-63fb-4a8a-93db-f460d678c2c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Table(name='flight_raw', catalog='workspace', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de0c8e7-8a10-4098-a601-0e1787af8579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>FL_DATE</td><td>date</td><td>null</td></tr><tr><td>OP_CARRIER</td><td>string</td><td>null</td></tr><tr><td>OP_CARRIER_FL_NUM</td><td>bigint</td><td>null</td></tr><tr><td>ORIGIN</td><td>string</td><td>null</td></tr><tr><td>ORIGIN_CITY_NAME</td><td>string</td><td>null</td></tr><tr><td>DEST</td><td>string</td><td>null</td></tr><tr><td>DEST_CITY_NAME</td><td>string</td><td>null</td></tr><tr><td>DEP_TIME</td><td>bigint</td><td>null</td></tr><tr><td>ARR_TIME</td><td>bigint</td><td>null</td></tr><tr><td>CRS_DEP_TIME</td><td>bigint</td><td>null</td></tr><tr><td>CRS_ARR_TIME</td><td>bigint</td><td>null</td></tr><tr><td>DISTANCE</td><td>bigint</td><td>null</td></tr><tr><td>CANCELLED</td><td>boolean</td><td>null</td></tr><tr><td>TAXI_IN</td><td>bigint</td><td>null</td></tr><tr><td>WHEELS_ON</td><td>bigint</td><td>null</td></tr><tr><td></td><td></td><td></td></tr><tr><td># Delta Statistics Columns</td><td></td><td></td></tr><tr><td>Column Names</td><td>CANCELLED, DEP_TIME, OP_CARRIER_FL_NUM, TAXI_IN, ORIGIN, DEST_CITY_NAME, ARR_TIME, DEST, CRS_DEP_TIME, OP_CARRIER, DISTANCE, WHEELS_ON, CRS_ARR_TIME, ORIGIN_CITY_NAME, FL_DATE</td><td></td></tr><tr><td>Column Selection Method</td><td>first-32</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td># Detailed Table Information</td><td></td><td></td></tr><tr><td>Catalog</td><td>workspace</td><td></td></tr><tr><td>Database</td><td>default</td><td></td></tr><tr><td>Table</td><td>flight_raw</td><td></td></tr><tr><td>Created Time</td><td>Sun Jan 11 08:43:06 UTC 2026</td><td></td></tr><tr><td>Last Access</td><td>UNKNOWN</td><td></td></tr><tr><td>Created By</td><td>Spark </td><td></td></tr><tr><td>Statistics</td><td>3258451 bytes, 300000 rows</td><td></td></tr><tr><td>Type</td><td>MANAGED</td><td></td></tr><tr><td>Location</td><td></td><td></td></tr><tr><td>Provider</td><td>delta</td><td></td></tr><tr><td>Owner</td><td>vivek.161b275@gmail.com</td><td></td></tr><tr><td>Is_managed_location</td><td>true</td><td></td></tr><tr><td>Predictive Optimization</td><td>ENABLE (inherited from METASTORE metastore_aws_us_east_2)</td><td></td></tr><tr><td>Table Properties</td><td>[delta.enableDeletionVectors=true,delta.feature.appendOnly=supported,delta.feature.deletionVectors=supported,delta.feature.invariants=supported,delta.minReaderVersion=3,delta.minWriterVersion=7]</td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "FL_DATE",
         "date",
         null
        ],
        [
         "OP_CARRIER",
         "string",
         null
        ],
        [
         "OP_CARRIER_FL_NUM",
         "bigint",
         null
        ],
        [
         "ORIGIN",
         "string",
         null
        ],
        [
         "ORIGIN_CITY_NAME",
         "string",
         null
        ],
        [
         "DEST",
         "string",
         null
        ],
        [
         "DEST_CITY_NAME",
         "string",
         null
        ],
        [
         "DEP_TIME",
         "bigint",
         null
        ],
        [
         "ARR_TIME",
         "bigint",
         null
        ],
        [
         "CRS_DEP_TIME",
         "bigint",
         null
        ],
        [
         "CRS_ARR_TIME",
         "bigint",
         null
        ],
        [
         "DISTANCE",
         "bigint",
         null
        ],
        [
         "CANCELLED",
         "boolean",
         null
        ],
        [
         "TAXI_IN",
         "bigint",
         null
        ],
        [
         "WHEELS_ON",
         "bigint",
         null
        ],
        [
         "",
         "",
         ""
        ],
        [
         "# Delta Statistics Columns",
         "",
         ""
        ],
        [
         "Column Names",
         "CANCELLED, DEP_TIME, OP_CARRIER_FL_NUM, TAXI_IN, ORIGIN, DEST_CITY_NAME, ARR_TIME, DEST, CRS_DEP_TIME, OP_CARRIER, DISTANCE, WHEELS_ON, CRS_ARR_TIME, ORIGIN_CITY_NAME, FL_DATE",
         ""
        ],
        [
         "Column Selection Method",
         "first-32",
         ""
        ],
        [
         "",
         "",
         ""
        ],
        [
         "# Detailed Table Information",
         "",
         ""
        ],
        [
         "Catalog",
         "workspace",
         ""
        ],
        [
         "Database",
         "default",
         ""
        ],
        [
         "Table",
         "flight_raw",
         ""
        ],
        [
         "Created Time",
         "Sun Jan 11 08:43:06 UTC 2026",
         ""
        ],
        [
         "Last Access",
         "UNKNOWN",
         ""
        ],
        [
         "Created By",
         "Spark ",
         ""
        ],
        [
         "Statistics",
         "3258451 bytes, 300000 rows",
         ""
        ],
        [
         "Type",
         "MANAGED",
         ""
        ],
        [
         "Location",
         "",
         ""
        ],
        [
         "Provider",
         "delta",
         ""
        ],
        [
         "Owner",
         "vivek.161b275@gmail.com",
         ""
        ],
        [
         "Is_managed_location",
         "true",
         ""
        ],
        [
         "Predictive Optimization",
         "ENABLE (inherited from METASTORE metastore_aws_us_east_2)",
         ""
        ],
        [
         "Table Properties",
         "[delta.enableDeletionVectors=true,delta.feature.appendOnly=supported,delta.feature.deletionVectors=supported,delta.feature.invariants=supported,delta.minReaderVersion=3,delta.minWriterVersion=7]",
         ""
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "col_name",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "data_type",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "comment",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 63
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "col_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "describe extended flight_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c3fe681-49c5-443f-98da-38b2f649b158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Distinct destination city**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e0b3ec-130d-4e04-b1b2-db475fae8c95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>distinct_destination_city</th></tr></thead><tbody><tr><td>Newark, NJ</td></tr><tr><td>Fort Wayne, IN</td></tr><tr><td>Dayton, OH</td></tr><tr><td>Columbia, SC</td></tr><tr><td>Boston, MA</td></tr><tr><td>Denver, CO</td></tr><tr><td>Helena, MT</td></tr><tr><td>Chattanooga, TN</td></tr><tr><td>Harrisburg, PA</td></tr><tr><td>San Antonio, TX</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Newark, NJ"
        ],
        [
         "Fort Wayne, IN"
        ],
        [
         "Dayton, OH"
        ],
        [
         "Columbia, SC"
        ],
        [
         "Boston, MA"
        ],
        [
         "Denver, CO"
        ],
        [
         "Helena, MT"
        ],
        [
         "Chattanooga, TN"
        ],
        [
         "Harrisburg, PA"
        ],
        [
         "San Antonio, TX"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "distinct_destination_city",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dist_city = flight_raw_df3.where(\"DEST_CITY_NAME is not null\")\\\n",
    "                         .select(expr(\"DEST_CITY_NAME as distinct_destination_city\"))\\\n",
    "                         .distinct()\n",
    "display(dist_city.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c8715fb-32d2-4b04-968b-924d8b0f6d21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**What were the most common destinations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d6f2117-d961-4ea1-9cf4-3129912ed42f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DEST_CITY_NAME</th><th>count</th></tr></thead><tbody><tr><td>Chicago, IL</td><td>18409</td></tr><tr><td>Atlanta, GA</td><td>14492</td></tr><tr><td>Dallas/Fort Worth, TX</td><td>13246</td></tr><tr><td>Los Angeles, CA</td><td>11151</td></tr><tr><td>Houston, TX</td><td>10092</td></tr><tr><td>Phoenix, AZ</td><td>9914</td></tr><tr><td>St. Louis, MO</td><td>9180</td></tr><tr><td>Washington, DC</td><td>8520</td></tr><tr><td>Detroit, MI</td><td>8171</td></tr><tr><td>New York, NY</td><td>7803</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Chicago, IL",
         18409
        ],
        [
         "Atlanta, GA",
         14492
        ],
        [
         "Dallas/Fort Worth, TX",
         13246
        ],
        [
         "Los Angeles, CA",
         11151
        ],
        [
         "Houston, TX",
         10092
        ],
        [
         "Phoenix, AZ",
         9914
        ],
        [
         "St. Louis, MO",
         9180
        ],
        [
         "Washington, DC",
         8520
        ],
        [
         "Detroit, MI",
         8171
        ],
        [
         "New York, NY",
         7803
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DEST_CITY_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "common_dest = flight_raw_df3.select(\"DEST_CITY_NAME\")\\\n",
    "                            .where(\"DEST_CITY_NAME is not null\")\\\n",
    "                            .groupBy(\"DEST_CITY_NAME\")\\\n",
    "                            .count()\\\n",
    "                            .orderBy(\"count\", ascending=False)\\\n",
    "                            .limit(10)\n",
    "display(common_dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c78a9c85-4902-4cf0-ba3b-6edb5c4f0381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**MAGIC COMMANDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12174861-1b9d-44c1-8c53-a58bcb29777d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core_bronze_dev\ncore_silver_dev\ndemo_catalog\ndev\nsamples\nsystem\nworkspace\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "ls /Volumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e611e053-4caf-492c-9bad-bce59e37c667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.session.SparkSession at 0xffde23392570>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%python\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af29b78f-ff94-46ef-a8ae-02f7fc434483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>catalog</th></tr></thead><tbody><tr><td>core_bronze_dev</td></tr><tr><td>core_silver_dev</td></tr><tr><td>demo_catalog</td></tr><tr><td>dev</td></tr><tr><td>samples</td></tr><tr><td>system</td></tr><tr><td>workspace</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "core_bronze_dev"
        ],
        [
         "core_silver_dev"
        ],
        [
         "demo_catalog"
        ],
        [
         "dev"
        ],
        [
         "samples"
        ],
        [
         "system"
        ],
        [
         "workspace"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "catalog",
            "nullable": false,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 68
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "catalog",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SHOW CATALOGS;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90c97e0d-b153-4f4c-818a-1902ad3398a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**detailed list of files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6bf0697-5574-4624-867f-1a66bc997dd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 124K\ndrwxrwxrwx 2 root root 4.0K Jan 11 13:33 14_days-AI_Challenge\n-rwxrwxrwx 1 root root 2.1K Aug 27 06:01 ch2_Assignment-Solutions-1.ipynb\n-rwxrwxrwx 1 root root 4.1K Aug 27 08:37 ch2_Assignment-Solutions-2.ipynb\n-rwxrwxrwx 1 root root 6.9K Aug 27 08:38 ch2_Assignment-Solutions-3.ipynb\n-rwxrwxrwx 1 root root  16K Aug 27 09:48 ch2_Assignment-Solutions-4-5.ipynb\ndrwxrwxrwx 2 root root 4.0K Jan 11 13:33 data\n-rwxrwxrwx 1 root root  23K Jan 11 13:33 Day_2_challenge.ipynb\ndrwxrwxrwx 2 root root 4.0K Jan 11 13:33 Drafts\ndrwxrwxrwx 2 root root 4.0K Jan 11 13:33 Mastering PySpark\ndrwxrwxrwx 2 root root 4.0K Jan 11 13:33 my_pipeline\n-rwxrwxrwx 1 root root  18K Aug 29 09:35 mypracticenotebook.ipynb\n-rwxrwxrwx 1 root root 2.4K Dec  8 18:44 New Query 2025-12-08 11:37pm.dbquery.ipynb\n-rwxrwxrwx 1 root root 4.3K Dec 16 09:07 PII_notebook.ipynb\ndrwxrwxrwx 2 root root 4.0K Jan 11 13:33 Preethi\n-rwxrwxrwx 1 root root  832 Jan 11 13:17 Python Practice.ipynb\n-rwxrwxrwx 1 root root 7.9K Oct 11 16:37 Untitled Notebook 2025-08-28 16:04:32.ipynb\n-rwxrwxrwx 1 root root 1.6K Oct  5 15:28 Untitled Notebook 2025-10-04 11:15:29.ipynb\n-rwxrwxrwx 1 root root 9.7K Dec 19 05:38 Untitled Notebook 2025-10-05 19:37:13.ipynb\n-rwxrwxrwx 1 root root  854 Jan 11 13:17 Untitled Notebook 2025-12-16 14:37:12.ipynb\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "ls -lh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d777a15-4149-4f01-892c-642fc871cc69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**change directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60059507-5e8a-4367-abdb-a78c82285f13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 1: cd: /Volumes/workspace/ecommerce/ecommerce_data: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "cd /Volumes/workspace/ecommerce/ecommerce_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "427ef125-c1bc-402c-81e4-9ac66b6461a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "mkdir test_folder    # creating directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "993bea65-1e8c-4ce0-8c70-63b0e21d2aca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "mkdir -p data/raw/2024    # creating nested directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83d03eea-e37d-4c37-aee3-8f3ac249ddd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "rm -r test_folder    # deleting directory recursively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17ba6e25-c371-4089-99d7-000222e1844c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Call Number\",\"Unit ID\",\"Incident Number\",\"Call Type\",\"Call Date\",\"Watch Date\",\"Received DtTm\",\"Entry DtTm\",\"Dispatch DtTm\",\"Response DtTm\",\"On Scene DtTm\",\"Transport DtTm\",\"Hospital DtTm\",\"Call Final Disposition\",\"Available DtTm\",\"Address\",\"City\",\"Zipcode of Incident\",\"Battalion\",\"Station Area\",\"Box\",\"Original Priority\",\"Priority\",\"Final Priority\",\"ALS Unit\",\"Call Type Group\",\"Number of Alarms\",\"Unit Type\",\"Unit sequence in call dispatch\",\"Fire Prevention District\",\"Supervisor District\",\"Neighborhooods - Analysis Boundaries\",\"RowID\",\"case_location\",\"data_as_of\",\"data_loaded_at\"\n\"160943727\",\"53\",\"16037460\",\"Medical Incident\",\"04/03/2016\",\"04/03/2016\",\"2016 Apr 03 11:15:12 PM\",\"2016 Apr 03 11:18:05 PM\",\"2016 Apr 03 11:18:33 PM\",\"2016 Apr 03 11:18:45 PM\",\"2016 Apr 03 11:35:10 PM\",\"2016 Apr 03 11:46:08 PM\",\"2016 Apr 04 12:11:46 AM\",\"Code 2 Transport\",\"2016 Apr 04 12:47:29 AM\",\"POLK ST/CEDAR ST\",\"San Francisco\",\"94109\",\"B04\",\"03\",\"3121\",\"2\",\"2\",\"2\",\"true\",\"Non Life-threatening\",\"1\",\"MEDIC\",\"2\",\"4\",\"6\",\"Tenderloin\",\"160943727-53\",\"POINT (-122.41983 37.786358)\",\"2024 Feb 05 03:27:52 AM\",\"2024 Feb 05 10:56:25 AM\"\n\"161021964\",\"AM14\",\"16040565\",\"Medical Incident\",\"04/11/2016\",\"04/11/2016\",\"2016 Apr 11 01:14:47 PM\",\"2016 Apr 11 01:19:53 PM\",\"2016 Apr 11 01:20:42 PM\",\"2016 Apr 11 01:21:06 PM\",\"2016 Apr 11 01:27:48 PM\",\"2016 Apr 11 02:14:47 PM\",\"2016 Apr 11 02:33:36 PM\",\"Code 2 Transport\",\"2016 Apr 11 03:06:20 PM\",\"VALENCIA ST/15TH ST\",\"San Francisco\",\"94103\",\"B02\",\"06\",\"5226\",\"2\",\"2\",\"2\",\"false\",\"Potentially Life-Threatening\",\"1\",\"PRIVATE\",\"1\",\"2\",\"9\",\"Mission\",\"161021964-AM14\",\"POINT (-122.42204 37.76654)\",\"2024 Feb 05 03:27:52 AM\",\"2024 Feb 05 10:56:25 AM\"\n\"160930738\",\"85\",\"16036742\",\"Structure Fire / Smoke in Building\",\"04/02/2016\",\"04/01/2016\",\"2016 Apr 02 07:48:03 AM\",\"2016 Apr 02 07:49:50 AM\",\"2016 Apr 02 07:50:12 AM\",\"2016 Apr 02 07:50:48 AM\",,,,\"Fire\",\"2016 Apr 02 07:55:35 AM\",\"TURK ST/DODGE ST\",\"San Francisco\",\"94102\",\"B02\",\"03\",\"1554\",\"3\",\"3\",\"3\",\"true\",\"Alarm\",\"1\",\"MEDIC\",\"10\",\"2\",\"6\",\"Tenderloin\",\"160930738-85\",\"POINT (-122.416985 37.78242)\",\"2024 Feb 05 03:27:52 AM\",\"2024 Feb 05 10:56:25 AM\"\n\"160931745\",\"E03\",\"16036856\",\"Medical Incident\",\"04/02/2016\",\"04/02/2016\",\"2016 Apr 02 01:08:02 PM\",\"2016 Apr 02 01:09:44 PM\",\"2016 Apr 02 01:10:16 PM\",\"2016 Apr 02 01:13:09 PM\",\"2016 Apr 02 01:15:07 PM\",,,\"Against Medical Advice\",\"2016 Apr 02 01:16:51 PM\",\"WILLOW ST/LARKIN ST\",\"San Francisco\",\"94109\",\"B02\",\"03\",\"1643\",\"3\",\"3\",\"3\",\"true\",\"Potentially Life-Threatening\",\"1\",\"ENGINE\",\"2\",\"2\",\"6\",\"Tenderloin\",\"160931745-E03\",\"POINT (-122.41762 37.78378)\",\"2024 Feb 05 03:27:52 AM\",\"2024 Feb 05 10:56:25 AM\"\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "head -5 /Volumes/dev/multi_datasets/spark_data/firedept.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11af04c2-18d8-46f1-add0-8e152c459ba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Call Number\",\"Unit ID\",\"Incident Number\",\"Call Type\",\"Call Date\",\"Watch Date\",\"Received DtTm\",\"Entry DtTm\",\"Dispatch DtTm\",\"Response DtTm\",\"On Scene DtTm\",\"Transport DtTm\",\"Hospital DtTm\",\"Call Final Disposition\",\"Available DtTm\",\"Address\",\"City\",\"Zipcode of Incident\",\"Battalion\",\"Station Area\",\"Box\",\"Original Priority\",\"Priority\",\"Final Priority\",\"ALS Unit\",\"Call Type Group\",\"Number of Alarms\",\"Unit Type\",\"Unit sequence in call dispatch\",\"Fire Prevention District\",\"Supervisor District\",\"Neighborhooods - Analysis Boundaries\",\"RowID\",\"case_location\",\"data_as_of\",\"data_loaded_at\"\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "grep \"Call Number\" /Volumes/dev/multi_datasets/spark_data/firedept.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "153cfd10-e2f6-4d92-8e71-ecdf0e7d3b92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "cp /Volumes/dev/multi_datasets/spark_data/firedept.csv \\\n",
    "   /Volumes/core_bronze_dev/my_bronze_schema/my_volume/\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7792313465935833,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_2_challenge",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}